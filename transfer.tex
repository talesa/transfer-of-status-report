\documentclass[12pt]{article}
\input{packages-and-defs}

\begin{document}
\input{title}

\includepdf[pages=-]{nami}

\title{Improving, Extending and Automating Inference Amortization for First Order Probabilistic Programs}
\date{}
\maketitle

\vspace{-90pt}

\section{Introduction}

In this part of the report, I introduce the background knowledge, review the recent literature around amortized inference and present future directions for this avenue of research. 
I also present a preliminary schedule for the rest of my DPhil programme.

The main themes of this work are improving, extending and automating inference amortization for first order probabilistic programs.
If any of these terms are unfamiliar to you, hold tight, we will introduce all of them in the next section.

\todo{insert beginning of Section 2 from Brooks paper into the transfer}
\todo{go through the reading group papers and make sure you include points from those}
\todo{do we have a proof that if inverse Gm introduces aditional assumptions then we cannot match the true posterior? it might be in one of Brooks' or Stuhlmueller's papers}
\todo{do we have solid argument we want want to tlimit the dimensionality of the factors? this might also be in one of Brooks' or Stuhlmueller's papers}
\todo{on one pass look for opportunities to support the narrative with code snippet examples and diagrams, especially of GMs}
\todo{prepare questions for Frank and Tom}
\todo{do we train a prior in VAE}
\todo{maybe do a page with notation like in Willie's dissertation?}

\section{Background and literature review}

\subsection{Probabilistic machine learning}
Probabilistic machine learning is a branch of machine learning which makes use of probabilistic modelling framework to reason about learning models and for incorporating uncertainty into that process \citep{Ghahramani2015}.
Its major advantages are the ability to incorporate prior knowledge by making explicit modelling assumptions, the ability to obtain well calibrated uncertainty estimates, and the interpretability of the models.
The latter is these days a very sought after characteristic, especially as machine learning is starting to pervade the highly regulated domains such as medicine, jurisprudence, finance and autonomous operation of robots, most notably - autonomous driving.
The price we pay for those advantages is the often computationally expensive process of learning as well as the risk of model misspecification by making inaccurate assumptions what results in learning inaccurate models.

Learning in probabilistic models boils down to the process called inference - determining the probability distribution of a quantity/information of interest based the prior belief and the data - which is the core problem in probabilistic machine learning.
In the mathematical terms - our probabilistic model defines a joint distribution $p(x,y)$ over the hidden (or latent) random variables (RVs) $x$ and the observed RVs $y$.
The prior beliefs are encoded in the model as so called prior distribution $p(x)$.
Most often focal point of probabilistic inference is determining the posterior $p(x|y)$ - the probability distribution over the hidden RVs $x$ conditioned on the observed value of $y$.
Bayes' Law, easily the most recognizable equation in bayesian statistics, dictates how to obtain the posterior $p(x|y) = p(x,y)/p(y)$.
The challenging part of the entirety of probabilistic inference is estimating the so-called evidence $p(y)$ what requires performing or approximating the integral $\int p(x,y) \text{d}x$.
Evaluating such high dimensional integrals can rarely be done analytically (except some special cases like linear gaussian models) and in general can be arbitrarily difficult.
This is what fundamental reason for why Bayesian inference is difficult - it is tightly related to the fundamental challenge of performing high dimensional integration. 
From now on when referring to `inference` we mean Bayesian inference, as contrasted to non probabilistic inference in other domains of machine learning.

This is however not the end of the story - 
what we are actually interested in is the probability distribution over some quantity of interest which is usually a function of the latent variables $g=f(x)$, e.g. a loss function.
The probability distribution of interest may hence be denoted as $p_g(g)$.
Often we only seek a point estimate, i.e. the expected value of that function under the posterior $\mu_g=\E_{g \sim p_g(g)}[g]=\E_{x \sim p(x|y)}[f(x)]$.

\todo{potentially the right column from TAL's diagram}
\todo{how to best denote the entire transformation above? look up Kosiorek's blog post}

We do have efficient inference methods for certain classes of models, however we lack general purpose inference methods that would be efficient for all the models and that is what often stops us from formulating and attempting to use more complex probabilistic 
models\footnote{In many of those cases inefficient inference implies that inference is infeasible at the scale we are interested in.}.

\subsection{Probabilistic programming}
Probabilistic programming is an effort which focuses on 
(1) creating systems that allow its users to specify probabilistic models in a flexible manner 
and 
(2) running inference on them in an automated way.
The way to achieve this is to abstract away the process of inference by disentangling it from the problem of modelling, which are usually quite tightly coupled.
% The model is specified by the user while the inference should be performed in automated way by the probabilistic programming backend inference engine with minimal input from the user.
Such approach carries several promises:
(1) it enables users with deep domain knowledge, but without extensive expertise in inference to use or at least prototype in the probabilistic modelling framework without the necessity to work hand in hand with a statistician,
(2) it allows for modularity and compositionality - the ability to perform probabilistic reasoning in complex, compound models consisting consisting of multiple exchangeable components.

Defining a probabilistic model is done by specifying a joint probability distribution $p(x,y)$ using a combination of 
(1) definitions of latent and observed random variables using probability distributions, 
(2) deterministic transformation applied to the sampled values of those random variables, 
and optionally (not all probabilistic programming languages (PPLs) allow that) 
(3) deterministic or even stochastic control flow constructs.  

To support automated inference in an unconstrained class of probabilistic models we need general purpose inference algorithms.
There exist a few algorithms that support inference in such universal probabilistic programming languages, more about it in \autoref{sec:inference-prob-prog} 
However even general purpose inference algorithm do not behave the same way on all models - their efficiency varies, some are better suited to particular class of models than others.
An ideal probabilistic programming system should be able to automatically select the most efficient of the inference methods in its arsenal for the given problem, including not only all of the general purpose algorithms but also more bespoke ones.
An example of such attempts is work by \citet{ZinkovEM} where they present a method to perform Expectation Maximization \citep{EM} if closed-form solutions can be statically discovered by the system compiler.
\todo{ask Rob for comments}

Some criticism of the idea or about feasibility of probabilistic programming revolves around the feeling that it is a form of dishonesty to promise users they will be able to specify arbitrary probabilistic models, but not providing the ability to run efficient inference in all of these models.
It is clear that there is a trade-off between the user's freedom of specifying the model and the efficiency of inference.
This leads to a spectrum of probabilistic programming systems - some of them limit the expressivity of the model specification language by appropriate design of language semantics to allow the use of an inference method particularly effective for such constrained class of models, while others strive to provide unconstrained expressivity at the cost of inference performance.
For this work the most relevant distinction of this kind is into the systems that focus on models equivalent in expressivity to Bayesian Networks (Bayes Nets, BNs), such as Stan \citep{Stan}, BUGS \citep{WinBUGS,BUGSproject} and its cousin JAGS \citep{JAGS}, or to Graphical Models (GMs) such as Infer.NET \citep{InferNET}, and so-called `universal probabilistic programming languages` built on top of existing Turing-complete programming languages such as Anglican \citep{anglican}, Church \citep{GoodmanEtAl2008} and Venture \citep{venture}.
Bayesian Networks and Graphical Models are ways to describe assumptions we make about the probability distribution over multiple random variables, to find out more have a look at \citep{KollerFriedman2009}.
The last category is also known as the higher-order probabilistic programming languages because they allow stochastic recursion, as contrasted to first-order probabilistic programming languages which expressivity from the probabilistic modelling perspective is equivalent to graphical models, i.e. they have finite and deterministic number of random variables.
This work, and particularly out contributions to amortized inference are mostly relevant to this class of models.

Recently there is also an upsurge in development of so-called deep probabilistic programming languages such as Edward \citep{TranEtAl2016}, Pyro \citep{Pyro2018} and Probtorch \citep{Siddharth2017} which main focus is on stochastic variational inference for deep generative models and hence from the probabilistic modelling perspective their expressivity is limited to graphical models.
\todo{this might be wrong, what is actually the class of models for those languages?}
% \todo{why there is so little in probabilistic programming for MRFs/undirected GMs? Infer.NET}

% Some say that users should not be cheated \footnote{Personal conversations with Michael Osborne}.

\todo{describe what first order probabilistic program is}



\subsection{Inference in probabilistic programming systems}
\label{sec:inference-prob-prog}

\todo{should I add explanations of what bias and variance are here?}

When discussing inference methods there is a variety of properties we are interested in.
The three important ones that need defining are 
(1)~bias -- the difference between this estimator's expected value and the true value of the parameter being estimated: 
$\E [\hat{\theta}] - \theta_0$, 
where $\hat{\theta}$ is a RV representing an estimator and $\theta_0$ is the true scalar value we are trying to estimate; 
an unbiased estimator is the one for which $\E [\hat{\theta}] = \theta_0$.
(2)~variance -- a measure of the average distance between the collection of estimates and the expected value of the estimates
${\displaystyle \operatorname {Var} ({\widehat {\theta }})=\E [({\widehat {\theta }}-\E [{\widehat {\theta }}])^{2}]}$;
(3)~consistency -- estimator converges in probability to the true value in the infinite sample size limit: 
$\forall \epsilon>0, \lim _{n\to \infty}\Pr {\big (}|\hat{X_{n}}-X_0| > \varepsilon {\big )}=0$, 
where $\hat{X_n}$ is a RV which represents an estimator using $n$ samples and $X_0$ is the true data generating distribution,
this property can be also thought of as estimator being asymptotically unbiased in the limit of infinite sample size.

Inference in probabilistic programming systems roughly divide into two broad categories: variational \citep{WainwrightJordan2008} or Monte Carlo based methods \citep{mcbook}.
The following description of the properties of these methods involves a grand dose of generalization.
Variational methods are usually not providing consistent estimators and their bias is hard to estimate, but are relatively quick.
Monte Carlo (MC) based methods usually provide consistent estimators, some methods provide unbiased estimators, and we can bound how the variance of the estimator decreases with the growing amount of samples what implies there is an informed way to trade precision for computational cost, unless we have to do with a pathological case of infinite variance. 
Markov chain Monte Carlo (MCMC) methods provide consistent estimators in the limit of the infinite sample size, but unlike MC methods they are generally biased after any fixed number of iterations \citep{JacobEtAl2017} and we cannot bound the variance in the same way we can do that for MC methods.
Both MC and MCMC methods are usually slower than the variational methods.

What is more in case of both MC based and variational methods the process of inference needs to be started again from scratch whenever new set of observed variables (i.e. dataset) $y$ is obtained and we want to obtain a posterior for that dataset.  

As mentioned earlier many of the probabilistic programming systems with limited expressivity of the models make such design choice to ensure the models are amenable to particular classes of inference algorithms. 
Best examples are STAN which expressivity is limited to Bayesian Networks with no discrete variables to be able to run use No-U-Turn-Sampler (NUTS) \citep{NUTS}, an improvement upon Hamiltonian Monte Carlo (HMC) \citep{HMC}, or Infer.NET which is limited to graphical models to be able to use variational methods such as variational message passing \citep{variationalmessagepassing} and expectation propagation \citep{EP} algorithms.

\todo{variational methods cannot really be applied to UPP, can they?}
There are several MC based general purpose inference algorithms that can be applied to higher order probabilistic programming systems, examples include Interacting Particle Markov Chain Monte Carlo \citep{rainforth2016interacting}, Particle Gibbs, Particle independent Metropolis-Hastings, or Sequential Monte Carlo \citep{WoodEtAl2014}.
One of the major challenges that those methods must accommodate for is the variable number of random variables between runs of the program because of the stochastic control flow.  

\todo{is below true?}
If we want to be able to quantify the error or uncertainty of our estimates we have to resort to using MC based methods.
However there exist applications for which existing MC methods are not fast enough to run inference at the required speed.
Some use-cases require close to real-time inference, few examples of such domains are finance, autonomous robot control and real-time medical diagnostics. 
All of them require well-calibrated uncertainty quantification.
We believe inference amortization is one of the possible solutions in these settings.




\subsection{Inference amortization}

Inference amortization is a method which allows to significantly accelerate run-time inference, wherein one looks to ``compile away'' the cost of inference
across different possible datasets
by learning an artifact that can be used to assist the inference process
at run time for a given dataset
\citep{StuhlmullerEtAl2013, VAE, ritchie2016deep, PaigeWood2016, LeEtAl2016, LeEtAl2017, FIVO, NaessethEtAl2017}.

Typically, this amortization
artifact takes the form of a parametrized proposal, $q(x ; \varphi(y; \eta))$, which takes
in data $y$ and regresses these to proposal parameters $\varphi(y; \eta)$, generally using
a deep neural network with parameters $\eta$.
For example, in practice for a continuous unbounded domain $\mathbb{R}$ 
a good, flexible proposal distribution could be for example a Gaussian 
$q(x ; \varphi(y; \eta)) = \mathcal{N}(x;\mu=\varphi_{\mu}(y; \eta),\sigma=\varphi_{\sigma}(y; \eta))$
or better -- a mixture of Gaussians
$q(x ; \varphi(y; \eta)) = \sum_k^K \varphi_{\alpha,k}(y; \eta) \times \mathcal{N}(x;\mu=\varphi_{\mu,k}(y; \eta),\sigma=\varphi_{\sigma,k}(y; \eta))$
where the neural network $\varphi$ would output 
the coefficient $\varphi_{\alpha,k}$,
the mean $\varphi_{\mu,k}$ and
the standard deviation $\varphi_{\sigma,k}$
for each of the $K$ components.

For consistency with the literature and to avoid clutter, we will often
use the shorthand $q(x|y)$ to represent this \emph{inference network}.
Though the exact process varies with context,
the inference network is usually trained either by drawing latent-data
sample pairs from a fixed joint distribution
$p(x,y)$~\citep{ritchie2016deep,PaigeWood2016,LeEtAl2016}, or 
as part of an Amortized Stochastic Variational Inference (ASVI) scheme~\citep{HoffmanEtAl2013,VAE,RezendeEtAl2014}.
Our work focuses mostly on the former of the two settings which we will call the inference compilation setting.
Once trained, it provides an efficient means of approximately
sampling from the posterior of a particular dataset.
We can then use importance sampling based methods to perform quick inference 
with respect to the posterior thanks to having good proposal distribution.

This approach allows us to utilize the high run-time performance of neural networks 
without sacrificing the interpretability offered by the probabilistic framework.

\todo{tie this somehow}

Once we have obtained $\mathcal{H}$, the next step is to use it to learn an inference network, $q_\eta(x|y)$, where $\eta$ are the parameters of the neural networks constituting the full inference network. 
For this, we use the factorization given by $\mathcal{H}$, that is, $q_\psi(x|y)=\prod^N_i q_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i))$, where $\text{Pa}_\mathcal{H}(x_i)$ stands for parents of the node $x_i$ according to graph $\mathcal{H}$.
For each factor $q_i(x_i\,|\,\text{Pa}(x_i))$, we must decide both the class of distributions for $x_i\,|\,\text{Pa}(x_i)$, and how the parameters for that class are calculated.
Once learned, we can both sample from, and evaluate the density of, the inference network for a given dataset by considering each factor in turn.


\section{Inference compilation}
\label{sec:inf-comp}

In this section we will focus on inference amoritzation in the inference compilation setting.

\todo{actually more verbose description of amortized inference with the objective etc}

\todo{make sure that somewhere you describe that in UPP you cannot have dependency inversion}

\todo{modify this figure to only describe existing inference compilation}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/diagram.pdf}
  \caption{
  Schematic diagram of inference compilation for probabilistic programs. 
  % Black elements refer to 
  Original diagram courtesy of \citet{LeEtAl2016}.}
  \label{fig:inf-amortization}
\end{figure}
\todo{describe the figure and tie it into the narrative}
\todo{overall my research agenda is to continue contributing to the automation of inference amortization along those 3 questions and other extensions like AMCI}




\section{Inference compilation vs ASVI}

In case of ASVI the model $p(x,y)$ is not fully specified by the user like in the inference compilation case
when user provides the entire probabilistic program specifying the details of all of the stochastic and deterministic
computation performed in the model.
Instead, the user specifies the graphical model, i.e. factorization, of $p(x)$ and $p(y|x)$, 
while the deterministic transformations of the sampled values are specified with a neural network with parameters $\theta$
what is usually denoted as $p_\theta(y|x)$.

The process of designing of the inference network is quite similar, but the more detailed our knowledge about the deterministic transformations inside the generative model the larger the opportunity we will be able to leverage it when designing an inference network.
We touch upon this topic in \autoref{sec:design}.




\section{Designing the inference network}
\label{sec:design}

After we have decided on our model $p(x,y)$ in inference compilation or $p_\theta(x)$ and $p_\theta(y|x)$ in ASVI setting we need to design our inference network. 
That requires several design decisions that we elaborate on below:\\
1. The factorization of the approximate posterior $q(x|y)$: deciding on the graphical model $\mathcal{H}$ of the approximate posterior what implies it factorizes as $q(x|y)=\prod^N_iq_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i))$, this is equivalent to choosing the variational family in the classical variational inference setting;\\
2. The proposal distribution: typically we have to decide on a parametric distribution for each one of the factors $q_i$, but instead we could use more flexible density estimators that do not constrain us to parametric distributions, e.g. normalizing flows \citep{RezendeMohamed2015}; \\
3. The function approximator: we need to decide how the parameters for the distribution or transformation in the normalizing flows are determined using a function approximator $\varphi$, usually this boils down to deciding about the architecture of a neural network we are using to regress parameters for each of the factors 
$q_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i)) = q_i(x_i\,;\,\varphi_i(\text{Pa}_\mathcal{H}(x_i); \eta))$.\\


\subsubsection*{Factorization of the approximate posterior $q(x|y)$}

\todo{intro from Brooks' or Stuhlmueller's paper on why we need an inverse. 
do they explain what factorization or graphical models are at all?}
\todo{maybe some figure here}
\todo{add some information how inverses were derived in the past?}

The factorization of the approximate posterior $q(x|y)$ reflect the conditional independence assumptions between random variables we impose on the approximate posterior.
The more assumptions we impose the easier the problem is computationally, but the less expressive our approximate posterior $q(x|y)$ becomes and hence we make a coarser approximation.
The fewer assumption we impose the more expressive our approximate posterior $q(x|y)$ is, but it becomes more expensive to train and use.
Those assumptions are conveniently expressed using a framework called graphical modelling \citep{KollerFriedman2009}.

Such perspectives makes us consider a spectrum of assumptions we can impose on the approximate posterior.
On one end of this spectrum lies the one of coarsest approximations we can make called the mean field assumption - assuming that the posterior factorizes into individual factors for each random variable $x_i$ without any dependencies between them, $q(x|y)=\prod^N_iq_i(x_i\,|\,y)$.
On the other end of this spectrum lies an autoregressive factorization which makes no assumptions since any joint distribution can always be factorized according to the probability chain rule $q(x|y)=\prod^N_iq_i(x_i\,|\,x_{<i},y)$.

What motivated the work in the first part of this report \citep{Webb2018} was the observation that if we impose more assumptions than those present in the generative model $p(x,y)$ we can make it impossible for our approximate posterior $q(x|y)$ to match the true posterior $p(x|y)$ no matter how flexible density estimator and how powerful function approximator we use.
In plain words the reason for this is that neural network that determines the parameters for the factor $q_i$ is not given all the necessary inputs (values of other random variables) to reconstruct the true posterior.
Even when our goal is not learning an accurate inference artifact but learning the model this effect is undesirable
% as limited capacity of the inference artifact forces the learnt model to adjust to this inaccurate inference hence limiting the capacity of the model itself, 
since because of the KL penalty term in the ELBO objective
$\mathcal{L}(y,q,p)=\log p(y)-KL(q(x|y) || p(x|y)) \le \log p(y)$ \citep{VAE}
optimizing under these assumptions tends to force the posterior $p_\theta(x|y)$ to satisfy the factorizing assumptions of the variational family $q$ \citep{FIVO}.

To alleviate that we introduced an algorithm that uses the graphical model $\mathcal{G}$ we assume for the generative model $p$ to search for a graphical model $\mathcal{H}$ for the approximate posterior which introduces as many assumptions as possible while not introducing any assumptions not present in the $p$, and hence not limit the expressivity of $q$ more than what is required to be able to match the true posterior.

We are not postulating that users should only use the factorization suggested by our method,
however we do recommend to use it as a guidance or a starting point before they decide to
introduce further assumptions into the inference network.

% Unlike mean-field and autoregressive factorizations, the factorizations produced by our method are very model-dependent and 
% we believe they provide more benefit to models with intricate graphical model structure.
% Compare VAE NaMI inverse which is just an autoregressive model with inverse of a binary tree.

% What is more - you might wonder why we would like to limit the number of random variables on each factor of the factorization - the goal of that is to limit the dimensionality of the conditioning set of RVs at each factor significantly.
% That translates into decreasing the dimensionality of the input to $\varphi_i$, we are analytically removing the inputs to the neural network that are unnecessary to determine the posterior.
% In case of structured models we would expect the dimensionality of the factors to be of the order $\mathcal{O}(1)$ as compared to $\mathcal{O}(N)$ for the autoregressive models where $N$ is the number of latent random variables in the model.
% We are not however circumventing one of the most significant issues of autoregressive models in general - our lack of ability to parallelize the computation what at the moment is one of the bottlenecks in using methods such as Inverse Autoregressive Flows \citep{IAF} and Masked Autoregressive Flows \citep{MAF}, and is the motivation for the development of involved training strategies to combine the strengths of both techniques such as in \citep{ParallelWavenet}.
% However the total dimensionality of the latent variables in a model is often smaller than the that of the observed variables and in inference amortization we are only targeting the latent variables so there is still hope for sufficient efficiency for practical purposes.

% Major issue with using autoregressive NNs is that they need O(N) passes of a neural network.
% One of the issues that may arise when using NaMI inverses is a similar problem.
% Sampling from the inference network takes O(N) where N is the total dimensionality of the latent variables.
% In contrast the mean field assumption allows us to sample in O(1).
% This is also the issue with MAF.
% Maybe some kind of Parallel Wavenet approach would work here?
% NaMI inverses should take less time than autoregressive factorization but they will still be of order O(N).

\todo{can you add figures here? think when reading the draft and design the figure}

\todo{do people add auxiliary RVs that are later marginalized like $\hat{y}$ suggested here at the bottom \url{http://pyro.ai/examples/_static/img/ss_vae_zoo.png}?}



\subsubsection*{Proposal distribution}

To learn a distribution for each of the factors $q_i$ we need to use density estimators, an individual one for each of the factors.
At the moment the most common approach in amortized inference is to use a parametric probability distribution with the parameters set by a neural network $\varphi_i(y)$.
The simplest choice one could make for the distribution of a random variable is to use the same distribution family as in the generative model.
However, in many cases this distribution might not be flexible enough to match the true posterior well.
For example, for the domain of $\mathbb{R}^d$ a mixture of gaussians is a more flexible choice than a single gaussian.

These choices are often quite ad-hoc and sometimes the true posterior might not match any parametric distribution well.
An alternative to using a parametric distribution might be a recently developed family of flexible density estimators called normalizing flows \citep{RezendeMohamed2015,IAF,MAF}.
Instead requiring the distribution to have a closed form density expression normalizing flows take a sample $z$ from some tractable distribution, commonly standard normal, and transform it using a bijective (i.e. invertible) transformation $f: \mathbb{R} \mapsto \mathbb{R}$ (or series of such transformations) into a much more complex, possibly multi-modal, random variable $x=f(z)$.
We can still evaluate the probability density at any given point $x$ since we are able to compute the determinant of the Jacobian and apply the change of variables formula between the source of randomness $z$ and the final variable $x$ according to 
$p_x(x) = p_z(z) \left|
    \mathrm{det} \frac{
      \partial f^{-1}
    }{
      \partial z\
    }
  \right|
  = p_z(z) \left|
    \mathrm{det} \frac{
      \partial f
    }{
      \partial z\
    }
  \right| ^{-1}$.
To make normalizing flows feasible function $f$ is chosen so that the determinant of the Jacobian can be easily computed (e.g. by constraining $f$ such that the Jacobian is triangular), and it is additionally parameterized by $\eta$ which are tuned such that the output random variable $x$ match the desired distribution. In practice a part of $f(z, \eta)$ is constituted by a neural network with parameters $\eta$.
We also have to be able to take the derivative of the density with respect to the parameters $\eta$ what is required to train such a density estimator.
Normalizing flows are thus a promising tool for learning flexible proposals for the $\mathbb{R}^d$ domain.

% In this case we are only interested in proposals over one dimensional random variables since the factorization is decided on separately by the NaMI algorithm and hence we are not subject to the inefficiency of autoregressive flows in sampling (MAF) or density evaluation of an arbitrary point that was not obtained by sampling from it (IAF).

However, distributions in some other domains which are also relatively frequent in probabilistic modelling such as natural numbers $\mathbb{N}$, double bounded continuous $[a,b]$ or bounded continuous $\mathbb{R}^+$, are not addressed by the promise of normalizing flows and hence might require development of better density estimators for those domains.
Currently they are often addressed by the following parametric distributions, respectively: Poisson, Kumaraswamy or Beta, and Gamma.

\todo{can you add figures here? think when reading the draft and design the figure}



\subsubsection*{Function approximator}
When using either the parametric distribution or normalizing flow as the proposal we need to choose the function approximator that is used to determine the parameters of the distribution or the transformation $f$ in normalizing flow.
In practice it amounts to choosing the architecture of a neural network.

The probabilistic program specifies all of the deterministic transformations applied to the samples of the random variables in the model
what creates an opportunity to leverage that information in designing the architecture of the neural network by attempting to invert the computation in the generative model \citep{TavaresEtAl2016,TavaresEtAl2017}.
It is yet unclear if this approach will prove of practical use in the inference compilation setting.

Another possible route to automate the selection of the architecture would be to look into the neural architecture search literature \citep{ElskenEtAl2018,ZophLe2017,pham18a}.

\todo{can you add figures here? think when reading the draft and design the figure}






% Methodology
% A good starting point for an attack on that problem is to limit the range of the models we will be able to run inference amortization on. One way to limit the range of models is to limit them to the finite-cardinality graphical models and attempt to invert their structure to guide the architecture of the neural network. That line of research was started by \citet{PaigeWood2016} and serves as our starting point that we plan to build upon.


% https://arxiv.org/pdf/1610.05735.pdf use the same ordering for their guide program as for the generative model as well as the same parametric family
% they also have some sort of optimization algorithm 
% 
% Tom
% >Unfortunately, there are two key stumbling blocks that often make it difficult for this idealized view of the Bayesian machine learning approach to be realized in practice. Firstly, a process known as Bayesian inference is required to solve the specified problems. This is typically a challenging task, closely related to integration, which is often computationally intensive to solve.
% 
% In terms of settings in which amortized inference is pursued there are 3 that should be named: 
% Bayesian Networks (BN) or Graphical Models (GM), i.e. models with deterministic number of random variables (RVs); 
% Deep Generative Models (DGMs) such as VAEs,
% Universal Probabilistic Programming.




\subsection{Amortized Monte Carlo Integration}
\label{sec:AMCI}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/diagram.pdf}
  \caption{
  Schematic diagram of inference compilation for probabilistic programs. 
  % Black elements refer to 
  Original diagram courtesy of \citet{LeEtAl2016}.}
  \label{fig:inf-amortization}
\end{figure}
\todo{describe the figure and tie it into the narrative}

Bayesian modeling is rooted in the calculation of expectations:
the eventual aim of modeling is typically to make a decision, or to construct
predictions for unseen data, both of which take the form of an expectation under
the posterior~\citep{robert2007bayesian}.  The aim of the vast majority
of Bayesian inference problems can thus be summarized in the form of one or more
expectations $\E_{p(x|y)}\! \left[f(x)\right]$, where $f(x)$ is a target function and
$p(x|y)$ is the posterior distribution on $x$ for some data $y$, 
which we typically only know up to a normalizing constant $p(y)$.  Sometimes $f(x)$
is not known up front, or we care about many different $f(x)$, such
that is convenient to just approximate $p(x|y)$ upfront, e.g. in the form of 
Monte Carlo samples, and then later use this to calculate estimates, rather than
address the target expectations directly.

However, it is often the case in practice that a particular target function, or class of
target functions, is known a priori.  
It has been well established in the literature that in such \emph{target-aware}
settings the aforementioned pipeline of first approximating $p(x|y)$ and then using this
as a basis for calculating $\E_{p(x|y)} \left[f(x)\right]$ is 
suboptimal as it ignores relevant information in
$f(x)$~\citep{mcbook,lacoste2011losscalibrated}.

Though there are many ways this relevant information can be 
exploited~\cite{hesterberg1988advances,wolpert1991monte,oh1992adaptive,evans1995methods,cerou2007adaptive,cerou2012sequential,
	de2005tutorial,lacoste2011losscalibrated,villen1994restart},
these are not currently utilized in probabilistic programming; systems
almost ubiquitously only target
the conditional distribution.  Many 
problems would thus benefit from extending the standard
probabilistic programming framework to an
\emph{integration programming} one which directly targets to the
calculation of expectations.

Here we focus on one particular setting in which
such target-aware systems could prove useful: amortized 
inference~\citep{ritchie2016deep,PaigeWood2016,LeEtAl2016}.
Specifically, we introduce AMCI, a new approach 
for \emph{amortizing Monte Carlo integration}, and show how
it might be applied in a probabilistic programming system.  The AMCI
estimator is based on importance sampling, but instead of relying
self-normalization, it uses the ratio of two separate estimators.
This allows it to achieve an arbitrary small error from single
sample estimates at test time, given sufficient
training and capacity of the amortization artifact.

Although is strongly motivated by Bayesian settings,
AMCI can be applied in any Monte Carlo integration setting.
Moreover, because
a generic integration can always be expressed
as an expectation through importance sampling, AMCI
allows amortizing integration more generally.

%\todo[inline]{AG: Tom, do you think more than \citep{lacoste2011losscalibrated,cobb2018losscalibrated} 
%as a reference in the line below is necessary? 
%What other works are you thinking about?}
%Although it is all too often overlooked, how to adjust for target-aware settings
%has been well established in the fixed-dataset context \citep{hesterberg1988advances,wolpert1991monte,oh1992adaptive,evans1995methods,lacoste2011losscalibrated}.
%In this paper, we extend these ideas to
%\emph{amortized} inference settings \citep{StuhlmullerEtAl2013, KingmaWelling2013, ritchie2016deep, PaigeWood2016, LeEtAl2016, LeEtAl2017, MaddisonEtAl2017, NaessethEtAl2017}, wherein one looks to amortize the cost of inference
%across different possible datasets
%by learning an artifact that assists the inference process
%at run time for a given dataset.

% \section{Method}

% Our first key contribution is in highlighting that the shortcomings of
% existing amortized inference approach for target-aware problems: even if the
% inference network manages to fully encapsulate the true posterior, the resulting
% estimator is still sub-optimal.  

% \vspace{-5pt}
% \section{Amortized Monte Carlo Integration}
% \label{sec:amci}

AMCI varies from standard amortized inference approaches in that: 
a) It operates in a target-aware fashion, incorporating
information about $f(x)$ into the amortization artifacts, increasing the efficiency 
at run time; b) Rather than
relying on self-normalized importance sampling (SNIS), AMCI amortizes and employs two separate proposals for estimating
the unnormalized target integral $\E_{p(x)}\!\left[f(x)p(y|x)\right]$ and the marginal
likelihood $\E_{p(x)}\!\left[p(y|x)\right]$; and c)
To account for cases in which multiple target functions may be
of interest, AMCI allows for amortization of parametrized functions $f(x ; \theta)$
by extending our target distribution
with a pseudo prior $p(\theta)$.  

%\vfill\clearpage

\textbf{Estimator} \quad The AMCI estimator is based around
taking the ratio of two 
separate non-self-normalized importance sampling estimators.
As each estimator can use a separate, individually tuned proposal, 
each can produce an arbitrarily low error when
$f(x)$ is either lower or upper bounded, allowing an arbitrary low
error in the overall estimate.
This is in contrast to standard SNIS, whose attainable 
error is lower bounded 
by $\E_{p(x|y)} [|f(x) - \E_{p(x|y)}[f(x)]|] ^2/N$
where $N$ is the number of samples~\citep{mcbook}.

To ensure all the samples are utilized for both expectations,
the full AMCI estimator is given by
\begin{flalign}
\frac{\E_{p(x)}\!\left[f(x)p(y|x)\right]}{\E_{p(x)}\!\left[p(y|x)\right]}
\! \approx \!
\frac{ \frac{\alpha}{N} \sum_n^N \!\frac{f(x_n) p(x_n, y) }{q_1(x_n|y)} \!+\! \frac{1-\alpha}{M} \sum_m^M \!\frac{f(x_m^*) p(x_m^*, y)}{q_2(x_m^*|y)}}
{\frac{\beta}{N} \sum_n^N \!\frac{p(x_n, y)}{q_1(x_n|y)} \!+\! \frac{1-\beta}{M} \sum_m^M \!\frac{p(x_m^*, y)}{q_2(x_m^*|y)}}
% \label{eq:estimator-combination}
\nonumber
\end{flalign}
where different sets of samples are drawn from 
\mbox{$x_n \!\sim\! q_1(x|y)$} and $x_m^* \!\sim\! q_2(x|y)$ respectively,
and $0 \!\le\! \alpha,\beta \!\le\! 1$.
If we had direct access to the optimal proposals, 
namely $q_1(x|y) \!\propto\! |f(x)|p(x|y)$ 
and $q_2(x|y) \!\propto\! p(x|y)$ \citep{mcbook}, it would
preferable to set $\alpha \!\!=\!\! 1$ and $\beta \!\!=\!\! 0$, leading to a zero-variance
estimator.  However, in practice, our proposals will not be perfect and
so using a convex combination of the two estimators allows us to
make use of all the samples at negligible extra cost.  
%It
%can be shown that asymptotically optimal parameter settings are 
%\mbox{$\alpha^* = N \big/ \left( (T-N) \frac{\Var[f(x_1) p(x_1,y)/q_1(x_1|y)]}{\Var[f(x_1^*) p(x_1^*,y)/q_2(x_1^*|y)]} + N \right)$}
%and
%\mbox{$\beta^*  = N \big/ \left( (T-N) \frac{\Var[p(x_1,y)/q_1(x_1|y)]}       {\Var[p(x_1^*,y)/q_2(x_1^*|y)]}          + N \right)$}.

 \textbf{Amortization} \quad
To evaluate this estimator, AMCI needs to learn to amortize proposals $q_1$ and $q_2$.  
These will take the form of a parametrized proposal, 
$q(x ; \varphi(y))$, which takes
in data $y$ and regresses these to proposal parameters 
$\varphi(y)$, using a deep neural network 
as per~\citep{ritchie2016deep,PaigeWood2016,LeEtAl2016}.
Following \citep{PaigeWood2016}, our objective for amortizing $q_2$ is
% distribution $p(x|y)$ is
\begin{flalign}
\mathcal{J}(\eta)
&= \mathbb{E}_{p(y)}\left[D_{KL} \left[ p(x|y) \mid\mid q_2(x;\varphi_2(y; \eta)) \right] \right] \nonumber \\
% &= \int D_{KL} \left[ p(x|y) \mid\mid q(x;\varphi(y; \eta)) \right] p(y)  \text{d}y \nonumber \\[3pt]
% &= \int p(x,y) \log \frac{p(x|y)}{q(x;\varphi(y; \eta))} \, \text{d}x \, \text{d}y \nonumber \\[3pt]
&=
\mathbb{E}_{p(x,y)}\left[- \log q_2(x ;\varphi_2(y; \eta))\right] 
+ \text{const wrt }\eta
% \label{eq:inf-comp:original}
\nonumber
\end{flalign}
This objective requires the ability to sample from $p(x,y)$ 
and it can be optimized using gradient methods.

\todo{explain what $\varphi_2$ is}

For amortizing $q_1$, we need to adjust this target to incorporate the effect of the
target function.  Further, when the target function is parameterized, i.e. $f(x;\theta)$,
we further allow amortization over functions by introducing a pseudo-prior $p(\theta)$ which
specifies which parameter values we wish to amortize over and their relative importance.
	We thus use the following objective for amortizing $q_1$
%Amortization over the space of targets requires modification to this objective
% and to our inference network, e.g. by also passing function parameters $\theta$
% as input, we can also amortize over 
% to allow amortization of different
% possible target functions, our inference network can now take in function parat
%  now also needs to take function parameters $\theta$ as input if we want
% to also amortize of different possible target functions.
%AMCI seeks a choice of $\eta$ which performs well across possible datasets $y$ and values of $\theta$.
%Hence it requires an expectation over $p(y)p(\theta)$ 
%%rather than just over $p(y)$, as in Eq.~\eqref{eq:inf-comp:original}.
%%This results in the following objective 
\begin{align}
&\mathcal{J}^{'}(\eta)
=
\mathbb{E}_{p(y)\,p(\theta)}\left[D_{KL} \left[ |f(x;\theta)| \, p(x|y) \mid\mid q_1(x;\varphi_1(y, \theta; \eta)) \right]\right] 
\nonumber \\
% &= \int D_{KL} \left[ |f(x;\theta)| \, p(x|y) \mid\mid q(x;\varphi(y, \theta; \eta)) \right] p(y) p(\theta) \, \text{d}y \, \text{d}\theta \nonumber \\[5pt]
% &= \int p(x,y) p(\theta) \, f(x;\theta) \log \frac{f(x;\theta)p(x|y)}{q(x;\varphi(y, \theta; \eta))} \, \text{d}x \, \text{d}y \, \text{d}\theta \nonumber \\[5pt]
&=
-\mathbb{E}_{p(x,y)\,p(\theta)}\left[-f(x; \theta) \log q_1(x ;\varphi_1(y, \theta; \eta))\right] + \text{const wrt }\eta
\nonumber
\end{align}
This expectation is over a tractable joint distribution $p(x,y)p(\theta)$ 
%and pseudo prior $p(\theta)$ as defined by a the modeller. 
%The entire objective 
and can also be optimized using gradient methods.

AMCI needs to learn both $q_1(x|y)$ using $\mathcal{J}^{'}$ above, 
as well as $q_2(x|y)$ using $\mathcal{J}$.
 % in Eq.~\eqref{eq:inf-comp:original}.
Both objectives can be jointly optimized using the same samples from $p(x,y)$.


\begin{figure}[t]
	% \vspace{-10pt}
  \centering
	\begin{subfigure}[b]{0.9\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/results_1d.pdf}
		\captionsetup{justification=centering}
		
	\end{subfigure}
	\caption{
		Results of amortization experiments for parameterized integrand $f(x; \theta)$.
		Rows represent examples for different values of $(y, \theta)$, respectively: $(-1, 1)$, $(-1.25, -0.5)$, $(-1.5, -2)$.
		% Uncertainty bars in columns 2, 3 are estimated over a 1000 runs.
		% Middle column errors are reported for 64 samples.
		For AMCI the number of samples from both proposals was equal, i.e. $M=N$.
		Columns respectively show: shapes of probability density functions, log mean error vs $\alpha$, 
		log mean error vs log number of samples for optimal proposal ($\alpha = 1$) and for posterior proposal ($\alpha = 0$). \vspace{-12pt}
	}
	\label{fig:results_1d}
\end{figure}

 \textbf{Experiment} \quad We demonstrate AMCI on an illustrative example
 and compare it with the SNIS estimator
 with the optimal proposal for approximating the posterior, further
 investigating the effects of varying $\alpha$ and $\beta$.
 % in Eq.~\eqref{eq:estimator-combination}.
We use the following toy model:
% \vspace{-20pt}
\begin{align*}
p(x) &= \mathcal{N}(x; 0, 1) &
q(x|y) &= \mathcal{N}\left(x; \varphi(\eta, y, \theta) \right) \\
p(y|x) &= \mathcal{N}(y; x, 1) &
f(x; \theta) &= \mathds{1}_{x>\theta} &
\end{align*} 
% \vspace{-20pt}
% \begin{array}
% {$p(x) = \mathcal{N}(x; 0, 1)$};
% {$p(y|x) = \mathcal{N}(y; x, 1)$};
% {$q(x|y) = \mathcal{N}\left(x; \varphi(\eta, y, \theta) \right)$};
% {$f(x; \theta) = \mathds{1}_{x>\theta}$};
% {$\theta \sim \text{Uniform}[-5,5]$}.
% \begin{array}
% We constrain the range of $\theta$ to be between -5 and 5, and we should not expect good performance if we wish to use an inference network trained on this model to evaluate expectation with $f(x;\theta)$ with $\theta$ outside of this range.
The posterior {$p(x|y) = \mathcal{N}(x; y/2, 1/2)$} and true value of
$\mathbb{E}_{p(x|y)} \left[ f(x; \theta) \right] = 1-\Phi(\theta)$ can be determined analytically, where $\Phi$ is a standard normal cumulative density function.

% \subsection{Results}
Results are presented in Fig.~\ref{fig:results_1d}. 
% The values of $y$ and $\theta$ plotted were chosen to be illustrative of the phenomena observed.
We found that fixing $\beta=0$ universally results in the smallest error of the estimates and hence we do not plot results on varying $\beta$.
AMCI performed better than SNIS when the optimal proposal differs from the posterior.
In some cases, when the optimal proposal differs greatly from the posterior the error for AMCI are over an order of magnitude smaller than for SNIS, see the first row.
However, when the optimal proposal is similar to the posterior, AMCI and SNIS give more similar performance.
%The reason for this is that AMCI draws samples separately both from the optimal proposal and posterior what in this case is clearly wasteful.
Here (third row) setting $\alpha < 1$ results in lower error.




\section{Research proposal}
There are several possible extensions and improvements to the existing inference amortization frameworks and systems. 
Below I list a few research directions that I am in good position to pursue.
The resulting projects vary in size, but each could yield a workshop or conference publication.
Some of them might end up being combined in a single paper for the purpose of publication.
I will provide more thorough description of each one below.
The list consists of the following themes and projects\\
\emph{Extensions to the inference compilation framework}\\
\textbf{A} \quad Amortized Monte Carlo Integration\\
\emph{Improvements and automation of inference compilation}\\
\textbf{B} \quad Normalizing flows as proposals for inference compilation\\
\textbf{C} \quad Choosing an inference network factorization in a data-driven way\\
% \textbf{E} \quad Making use of the information about the deterministic computations in the Bayesian networks and probabilistic programs\\
% \textbf{F} \quad Better ways of sampling from the model for the purpose of inference amortization\\
\emph{Applications of amortized inference in engineering, medicine and physical sciences}\\
\textbf{D} \quad Applications showcasing benefits of NaMI for models with rich structure\\




\subsubsection*{A \quad Amortized Monte Carlo Integration}
I am currently working on project A, Amortized Monte Carlo Integration, described in \autoref{sec:AMCI}.
We already have preliminary results published at UAI 2018 workshop on Uncertainty in Deep Learning \citep{golinski2018uai} and at International Conference on Probabilistic Programming 2018 \citep{golinski2018probprog} (without proceedings).
Current plan is to perform additional experiments to showcase the utility of the method, establish a process to tune the hyperparameters and submit the work to AIStats 2018, and in case of rejection to ICML 2019.
 
% Since this project is in further stage of the development more details about it will follow in the next section.


% \subsubsection*{B \quad Probabilistic programming semantics for integration}
% Project A naturally leads to project B, establishing semantics for performing integration in probabilistic programming systems.
% % We present first thoughts and suggestions in \citep{golinski2018probprog}.
% % The semantics we propose are based on the semantics and program transformations introduced by \citet{rainforth2016bopp}.
% Specifically, we consider 
% how one might support it in Anglican~\citep{anglican}.
% Based on the query macro \defopt of~\citep{rainforth2016bopp}, 
% we introduce \clj{defint}.  Like \defopt, \clj{defint} adjusts Anglican's
% standard query macro, \defquery, by providing an series of symbols as an additional input.
% The role of this input is to identify the variables in the program which
% are function parameters $\theta$.  The value of the 
% function $f(x;\theta)$ is then given by the output of the query.
% We can thus express our previous
% example as follows
% \begin{lstlisting}[basicstyle=\ttfamily\small,frame=none]
% (defint example [y] [$\theta$]
%  (let [x (sample (normal 0 1))
%        $\theta$ (sample (uniform-continuous -5 5))]
%   (observe (normal x 1) y)
%   (> x $\theta$)))
% \end{lstlisting}
% The conditional distribution of this query corresponds to the distribution
% $p(x|y)p(\theta)$, while its output represents $f(x;\theta)$.  It thus
% contains all the information we require to carry out AMCI by estimating
% gradients of $\mathcal{J}$ and $\mathcal{J}'$:
% we can sample from $p(x,y)p(\theta)$ and have evaluations of
% $f(x;\theta)$ at these points.  Therefore by coupling the AMCI approach
% with the inference compilation approach introduced for Anglican
% in~\cite{LeEtAl2016}, we have everything required for
% training an ``integration compilation'' artifact.
% 
% The syntax used to identify which variables correspond to
% $\theta$ is necessary because
% a) we do not need to learn a proposal for $\theta$ and b) at test time, we need
% a way to fix the value of $\theta$.  The latter of this is simply dealt
% with using the maximum marginal likelihood transformation applied for \defopt
% in~\cite{rainforth2017thesis}.   This effectively removes the
% \sample statements for $\theta$, replacing them with identity functions
% and making $\theta$ an input to the program.  With this transformation and
% the appropriately adapted trained amortized proposals, we thus
% have a means of automatically applying AMCI at test time.


\subsubsection*{B \quad Normalizing flows as proposals for inference compilation}
Project C revolves around using normalizing flows as more flexible proposal distributions for inference compilation, as described in \autoref{sec:design}.
The most prevalent and challenging type of random variable to formulate a proposal for is a continuous variable with an unbounded support.
Current practice is to use a multimodal distribution such as Mixture of Gaussians with a fixed number of components $K$.
This choice may fail in several ways - 
(1) there is no way for the proposal to match the true posterior if the posterior contains more than $K$ modes,
(2) the individual modes might not be very well approximated by a Gaussian, e.g. when they are highly asymmetric,
(3) the tails of the distribution might be heavier than those of Gaussian distribution.
Any of those scenarios might easily lead to a large (or even infinite) variance of our estimator, and hence to significant decrease in its sample efficiency.
An increasingly viable alternative to this approach is the growing family of flexible neural density estimation methods based on the idea of normalizing flow \citep{RezendeMohamed2015}.
I am planning to evaluate the utility of these methods, especially in the context of AMCI. 
% Currently the major difficulty of using normalizing flows for this purpose lies in the fact that methods allow either fast probability density evaluation at some data point $y$ (which is necessary for training) or the ability to efficiently sample (which is necessary at runtime).
% There are some attempts to circumvent this issue such as ParallelWavenet \citep{ParallelWavenet}, but anecdotal evidence suggests that approach is difficult to tune and train.
% Although I do not know of any successful attack on this problem yet I still deem it worthy to evaluate the utility of neural density estimators for amortized inference and its effect on sample efficiency of the final estimates even if we are not yet able to realize wall-time efficiency improvements in this way yet.

\subsubsection*{C \quad Choosing an inference network factorization in a data-driven way}
This project is a potential extension to the Natural Minimal Inverse (NaMI) method of inverting graphical models introduced by us in \citep{Webb2018}.
At the moment NaMI algorithm is forced to choose among multiple plausible faithful inverses of the generative graphical model and we use  heuristics to choose decide between those in principal equally faithful options.
% At the moment NaMI computes topological or reverse-topological sort orders, solves the draws between different orderings according to min-fill heuristic (i.e. minimizes the size of the cliques in the graph), computes two inverses and suggests to evaluate the performance of both.
My idea is to extend our current method to pick the factorization in a data-driven rather than heuristic way.
At a high level the new method would draw samples from the generative model and then use kernel density estimation to examine the form of the factors under different plausible factorizations. 
The `simpler` the forms of the factors are the easier they are to target with proposal distributions.
If a suitable measure of simplicity of a factor distribution is established it might be possible to further automate the process of choosing a suitable inference network factorization.


\subsubsection*{D \quad Applications showcasing the benefits of NaMI for models with rich structure}
At the moment we are lacking compelling examples showcasing the benefits of using our NaMI method.
We expect that our method yields most benefits for models with rich structure, and such can be found in the domains where
modelling is a prevalent tool of practice.
My plan is to seek suitable applications by looking for collaborators in engineering, finance, medicine or physical sciences.


\subsection{Preliminary schedule}
The following is a rough schedule for the rest of my DPhil degree, the milestones are pieces of work I intend to complete for relevant machine learning conferences.

\begin{table}[h!]
\begin{tabular}{lp{11cm}}
Present - October 2019         & Further work on projects A\&B, submission to AIStats \\
October 2018 - February 2019   & In case of rejection or delay further work on A\&B for submission to ICML, or investigation of project C \\
February 2019 - May 2019       & Work on C or/and D depending on which one holds more promise, submission to NIPS or partial work to ICML workshop \\
May 2019 - May 2020            & Reevaluation of available avenues for research and picking a new project \\
June 2020 - October 2020       & Internship \\
October 2020 - December 2020   & Writing up thesis
\end{tabular}
\end{table}





\newpage 
\bibliographystyle{plainnat}
\bibliography{bibliography/bibliography} 

\end{document}
