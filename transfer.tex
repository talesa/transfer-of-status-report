\documentclass[12pt]{article}
\input{packages-and-defs}

\begin{document}
\input{title}

\includepdf[pages=-]{nami}

\title{Improving, Extending and Automating\\Inference Amortization}
\date{}
\maketitle

\vspace{-50pt}


\section{Introduction}
The first part of the report is a manuscript ``Faithful Inversion of Generative Models for Effective Amortized Inference`` where we introduce NaMI: Natural Minimal I-Map algorithm.
I have contributed to that work by coding, running, plotting and analyzing Gaussian Mixture Model experiment in the Section 3.3, as well as another experiment that eventually did not make it to the manuscript. 
I have also participated in the ideation when choosing and designing the experiments and the process of writing and reviewing the manuscript itself.

In this part of the report, I introduce the background knowledge, review the literature around amortized inference and 
present my research directions in this domain. 
I also present a preliminary schedule for the rest of my DPhil programme.




\section{Background and literature review}

\subsection{Probabilistic machine learning}
Probabilistic machine learning is a branch of machine learning which makes use of probabilistic modelling framework to reason about learning models and for incorporating uncertainty into that process \citep{Ghahramani2015}.
Its major advantages are the ability to incorporate prior knowledge by making explicit modelling assumptions, the ability to obtain well calibrated uncertainty estimates, and the interpretability of the models.
The latter is these days a very sought after characteristic, especially as machine learning is starting to pervade the highly regulated domains such as medicine, jurisprudence, finance and autonomous operation of robots, most notably -- autonomous driving.
The price we pay for those advantages is the often computationally expensive process of learning as well as the risk of model misspecification by making inaccurate assumptions what results in learning inaccurate models.

Learning in probabilistic models boils down to the process called inference -- determining the probability distribution of a quantity/information of interest based the prior belief and the data -- which is the core problem in probabilistic machine learning.
In the mathematical terms -- our probabilistic model defines a joint distribution $p(x,y)$ over the hidden (or latent) random variables (RVs) $x$ and the observed RVs $y$.
The prior beliefs are encoded in the model as so called prior distribution $p(x)$.
Most often the focal point of probabilistic inference is determining the posterior $p(x|y)$ -- the probability distribution over the hidden RVs $x$ conditioned on the observed value of $y$.
Bayes' Law, easily the most recognizable equation in Bayesian statistics, dictates how to obtain the posterior $p(x|y) = p(x,y)/p(y)$.
The challenging part of the entirety of probabilistic inference is estimating the so-called evidence $p(y)$ what requires performing or approximating the integral $\int p(x,y) \text{d}x$.
Evaluating such high dimensional integrals can rarely be done analytically (except some special cases like linear Gaussian models) and in general can be arbitrarily difficult.
This is the fundamental reason for why Bayesian inference is difficult -- it is tightly related to the fundamental challenge of performing high dimensional integration. 
From now on when referring to `inference` we mean Bayesian inference, as contrasted to non probabilistic inference in other domains of machine learning.

% This is not the end of the story -- 
Often the posterior, or samples representing the posterior, are just an intermediate goal to
obtain the probability distribution over some quantity of interest which is a function of the latent variables $g=f(x)$, e.g. a loss function.
The probability distribution of interest may hence be denoted as $p_g(g|y)$.
Most often we only seek a point estimate, i.e. the expected value of that function under the posterior $\mu_g=\E_{g \sim p_g(g|y)}[g]=\E_{x \sim p(x|y)}[f(x)]$.

We do have efficient inference methods for certain classes of models, however we lack general purpose inference methods that would be efficient for all the models what often stops us from using more complex probabilistic models.
In many of those cases inefficient inference implies that inference is simply completely infeasible at the scale we are interested in.
That is the case because algorithms tend to scale unfavorably with the number of data points or dimensions of the problem.
In the case of big data even linear scaling in the number of data points is difficult to cope with and so we are looking for methods that can do with subsampling a part of the data set at each step - this gave rise to e.g. stochastic variational inference methods \citep{HoffmanEtAl2013}. 

The topic of efficiency in inference is very broad.
There are different aspects of efficiency - the two we consider worth pointing out here are the statistical and wall-time efficiency,
none of them is precisely defined.
In the context of statistical efficiency being more efficient may refer to many things, for example: 
when comparing Monte Carlo (MC) methods an estimator having smaller variance then another for a particular sample size is a more statistically efficient one, or
when comparing Markov chain Monte Carlo (MCMC) algorithms the one having less correlation between consecutive samples than the other is a more statistically efficient one.
However we also need to take into account the computational efficiency of a method - what if the advantage of a higher statistical efficiency of an MCMC method is offset by the fact that generating a sample is more computationally expensive than for less statistically efficient method?
What if we can further parallelize the less statistically efficient method?
Suddenly our measure of total efficiency becomes a measure along the lines of `number and autocorrelation of the samples per second under the computational budget of 10CPUs`.
Yet another aspect of the efficiency of a method is how much user's effort and attention is required to understand it, implement it and tune it (e.g. choose a proposal), although this one is usually not used under the umbrella term `efficiency`,\\
In this work we will use the word `efficient` relatively vaguely, assessing how a given method behaves in common contemporary problems.




\subsection{Probabilistic programming}
Probabilistic programming is an effort which focuses on 
(1) creating systems that allow its users to specify probabilistic models in a flexible manner 
and 
(2) running inference on them in an automated way.
The way to achieve this is to abstract away the process of inference by disentangling it from the problem of modelling, which are usually quite tightly coupled.
% The model is specified by the user while the inference should be performed in automated way by the probabilistic programming backend inference engine with minimal input from the user.
Such approach carries several promises:
(1) it enables users with deep domain knowledge, but without extensive expertise in inference to use the probabilistic modelling framework without the necessity to work hand in hand with a statistician,
(2) it allows for modularity and compositionality -- the ability to perform probabilistic reasoning in complex, compound models consisting consisting of multiple exchangeable components/submodules.

Defining a probabilistic model is done by specifying a joint probability distribution $p(x,y)$ using a combination of 
definitions of latent and observed random variables using probability distributions, 
deterministic transformation applied to the sampled values of those random variables, 
and optionally (not all probabilistic programming languages 
% (PPLs)
allow that) 
deterministic or even stochastic control flow constructs.  

To support automated inference in any probabilistic model we need general purpose inference algorithms.
% There exist a few algorithms that support inference in such universal probabilistic programming languages, more about it in \autoref{sec:inference-prob-prog} 
Even though a few general purpose inference algorithms exist, they do not behave the same way on all models -- their efficiency varies, some are better suited to particular classes of models than others.
An ideal probabilistic programming system should be able to automatically select the most efficient of the inference methods in its arsenal for the given problem, including not only all of the general purpose algorithms but also more bespoke ones, 
e.g. if compiler could detect inference in a given model can be run using the expectation maximization algorithm \citep{EM} it should be able to deploy it and yet again, if it is possible derive closed-form solutions for individual steps \citep{Zinkov2017}.

One of the criticisms of probabilistic programming is that it is dishonest to promise users they will be able to specify arbitrary probabilistic models, but not providing the ability to run efficient inference in all of these models.
It is clear that there is a trade-off between the user's freedom to specify arbitrary models and the efficiency of inference.
This leads to a spectrum of probabilistic programming systems -- some of them limit the expressivity of the model specification language by appropriate design of language semantics to allow the use of an inference method particularly effective for such constrained class of models, while others strive to provide unconstrained expressivity, possibly at the cost of inference performance.

For this work the most relevant distinction of this kind is into the systems that limit their expressivity to models with finite and deterministic cardinality (the number of random variables is finite and constant between samples from the model), such as Infer.NET \citep{InferNET}, Stan \citep{Stan}, BUGS \citep{WinBUGS,BUGSproject} or JAGS \citep{JAGS}, 
and so-called `universal probabilistic programming languages`, built on top of existing Turing-complete programming languages, such as Anglican \citep{anglican}, Church \citep{GoodmanEtAl2008}, WebPPL \citep{GoodmanStuhlmuller2014} and Venture \citep{venture},
which allow denotation of models with infinite, stochastic cardinality.
This work is mostly focusing on the former class of models.

Recently there is an upsurge in the development of so-called deep probabilistic programming libraries such as Edward \citep{TranEtAl2016}, Pyro \citep{Pyro2018} and Probtorch \citep{Siddharth2017} which main focus is on stochastic variational inference for deep generative models, and hence they live in programming environments which allow simple access to auto-differentiation necessary for training neural networks.

Our focus in this work is mostly on approximate inference using Monte Carlo (MC), Markov chain Monte Carlo (MCMC) and variational methods but it is also worth mentioning that there exist probabilistic programming systems which allow or even focus on exact inference using variable elimination, combinatorial search as well as approximate inference using message passing, and others.




\subsection{Inference in probabilistic programming systems}
\label{sec:inference-prob-prog}

When discussing approximate inference methods there is a variety of properties we are interested in.
The three important ones that need defining are 
(1)~bias -- the difference between this estimator's expected value and the true value of the parameter being estimated: 
$\E [\hat{\theta}] - \theta_0$, 
where $\hat{\theta}$ is a RV representing an estimator and $\theta_0$ is the true scalar value we are trying to estimate,
an unbiased estimator is the one for which $\E [\hat{\theta}] = \theta_0$;
(2)~variance -- a measure of the average square distance between the collection of estimates and the expected value of the estimates
${\displaystyle \operatorname {Var} ({\widehat {\theta }})=\E [({\widehat {\theta }}-\E [{\widehat {\theta }}])^{2}]}$;
(3)~consistency -- estimator converges in probability to the true value in the infinite sample size limit: 
$\forall \epsilon>0, \lim _{n\to \infty}\Pr {\big (}|\hat{X_{n}}-X_0| > \varepsilon {\big )}=0$, 
where $\hat{X_n}$ is a RV which represents an estimator using $n$ samples and $X_0$ is the true data generating distribution,
this property can be also thought of as estimator being asymptotically unbiased in the limit of infinite sample size.

Our attention is mostly on two broad categories of probabilistic inference algorithms: variational \citep{WainwrightJordan2008} or Monte Carlo based methods \citep{mcbook}.
The following description of the properties of these methods involves a grand dose of generalization.
Variational methods are usually not providing consistent estimators and their bias is hard to estimate, but are relatively quick.
Monte Carlo based methods usually provide consistent estimators, some methods provide unbiased estimators, and we can bound how the variance of the estimator decreases with the growing amount of samples what implies there is an informed way to trade precision for computational cost.
Markov chain Monte Carlo methods provide consistent estimators in the infinite sample size limit, but unlike MC methods they are generally biased after any fixed number of iterations \citep{JacobEtAl2017} and we cannot bound the variance in the same way we can for MC methods.
Both MC and MCMC methods are usually slower than the variational methods.
What is more in case of both MC based and variational methods the process of inference needs to be started again from scratch whenever a new set of observed variables (i.e. dataset) $y$ is obtained and we want to determine the posterior for that dataset.  

As mentioned earlier, many of the probabilistic programming systems with limited expressivity of the models make such design choice to ensure the models are amenable to particular classes of inference algorithms. 
Best examples are STAN which expressivity is limited to Bayesian Networks with no discrete variables to be able to run use No-U-Turn-Sampler-like algorithm \citep{NUTS}, an adaptive variant of Hamiltonian Monte Carlo (HMC) \citep{HMC}, or Infer.NET which is limited to factor graphs to be able to use variational methods such as variational message passing \citep{variationalmessagepassing} and expectation propagation \citep{EP} algorithms.

There are several MC based general purpose inference algorithms that can be applied to higher order probabilistic programming systems, examples include Interacting Particle Markov Chain Monte Carlo \citep{rainforth2016interacting}, Particle Gibbs, Particle independent Metropolis-Hastings, or Sequential Monte Carlo \citep{WoodEtAl2014}.
One of the major challenges that those methods must accommodate for is the varying number of random variables between the runs of the program because of the stochastic control flow.  

If we want to be able to quantify the uncertainty of our estimates we have to resort to using MC based methods.
However there exist applications for which existing MC methods are not fast enough to run inference at the required speed.
Some use-cases require close to real-time inference, few examples of such domains are finance, autonomous robot control and real-time medical diagnostics. 
All of them require well-calibrated uncertainty quantification.
We believe inference amortization is one of the possible solutions in these settings.




\subsection{Inference amortization}

Inference amortization is a method which allows to significantly accelerate run-time inference, wherein one looks to amortize the cost of inference across different possible datasets
by learning a
function approximator approximating the posterior for a given dataset
that can be used to assist the inference process
\citep{StuhlmullerEtAl2013, VAE, RitchieEtAl2016, PaigeWood2016, LeEtAl2016, LeEtAl2017, FIVO, NaessethEtAl2017}.

Typically, this amortization
artifact takes the form of a parametrized proposal, $q(x ; \varphi(y; \eta))$, which takes
in data $y$ and regresses these to proposal parameters $\varphi(y; \eta)$, usually using
a deep neural network with parameters $\eta$.
For example, if $x \in \mathbb{R}$ 
the proposal distribution could be a Gaussian 
$q(x ; \varphi(y; \eta)) = \mathcal{N}(x;\mu=\varphi_{\mu}(y; \eta),\sigma=\varphi_{\sigma}(y; \eta))$
or a mixture of Gaussians
$q(x ; \varphi(y; \eta)) = \sum_k^K \varphi_{\alpha,k}(y; \eta) \times \mathcal{N}(x;\mu=\varphi_{\mu,k}(y; \eta),\sigma=\varphi_{\sigma,k}(y; \eta))$
where the neural network $\varphi$ would output 
the coefficient $\varphi_{\alpha,k}$,
the mean $\varphi_{\mu,k}$ and
the standard deviation $\varphi_{\sigma,k}$
for each of the $K$ components.

% For consistency with the literature and to avoid clutter, we will often use the shorthand $q(x|y)$ to represent this \emph{inference network}.
Though the exact process varies with context,
the inference network is usually trained either by drawing latent-data
sample pairs from a fixed joint distribution
$p(x,y)$~\citep{RitchieEtAl2016,PaigeWood2016,LeEtAl2016}, or 
as part of an Amortized Stochastic Variational Inference (ASVI) scheme~\citep{HoffmanEtAl2013,VAE,RezendeEtAl2014}.
Our work focuses mostly on the former of the two settings which we will call the inference compilation setting.
Once the inference network is trained, it provides an efficient means of approximately
sampling from the posterior of a particular dataset.
We can then use importance sampling based methods to estimate expectations
with respect to the posterior thanks to having a reasonable proposal distribution.

This approach allows us to utilize the high run-time performance and large capacity of neural networks 
without sacrificing the interpretability offered by the probabilistic framework.
 
% Once we have obtained $\mathcal{H}$, the next step is to use it to learn an inference network, $q_\eta(x|y)$, where $\eta$ are the parameters of the neural networks constituting the full inference network. 
% For this, we use the factorization given by $\mathcal{H}$, that is, $q_\psi(x|y)=\prod^N_i q_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i))$, where $\text{Pa}_\mathcal{H}(x_i)$ stands for parents of the node $x_i$ according to graph $\mathcal{H}$.
% For each factor $q_i(x_i\,|\,\text{Pa}(x_i))$, we must decide both the class of distributions for $x_i\,|\,\text{Pa}(x_i)$, and how the parameters for that class are calculated.
% Once learned, we can both sample from, and evaluate the density of, the inference network for a given dataset by considering each factor in turn.


% \subsection{Inference compilation}
% \label{sec:inf-comp}
% 
% In this section we will focus on inference amoritzation in the inference compilation setting.
% 
% \todo{actually more verbose description of amortized inference with the objective etc}
% 
% \todo{make sure that somewhere you describe that in UPP you cannot have dependency inversion}
% 
% \todo{modify this figure to only describe existing inference compilation}
% 





% \subsection{Inference compilation vs ASVI}
% 
% In case of ASVI the model $p(x,y)$ is not fully specified by the user like in the inference compilation case
% when user provides the entire probabilistic program specifying the details of all of the stochastic and deterministic
% computation performed in the model.
% Instead, the user specifies the graphical model, i.e. factorization, of $p(x)$ and $p(y|x)$, 
% while the deterministic transformations of the sampled values are specified with a neural network with parameters $\theta$
% what is usually denoted as $p_\theta(y|x)$.
% 
% The process of designing of the inference network in both settings is similar, 
% but the more detailed our knowledge about the deterministic computation inside the generative model the larger is the opportunity we will be able to leverage it when designing an inference network.
% We touch upon this topic in \autoref{sec:design}.




\subsection{Designing the inference network}
\label{sec:design}

After we have decided on our model $p(x,y)$ in the inference compilation setting or $p(x)$ and $p_\theta(y|x)$ in the ASVI setting we need to design our inference network. 
Here, we will discuss the decisions required when designing the inference network for finite, deterministic cardinality models.
% 1. The factorization of the approximate posterior $q(x|y)$: we need to decide about the graphical model $\mathcal{H}$ of the approximate posterior what implies it factorizes as $q(x|y)=\prod^N_i q_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i))$, this is equivalent to choosing the variational family in the classical variational inference setting;\\
% 2. The proposal distribution: typically we have to decide on a parametric distribution for each one of the factors $q_i$, but instead we could use more flexible density estimators that do not constrain us to parametric distribution families, e.g. normalizing flows \citep{RezendeMohamed2015}; \\
% 3. The function approximator: we need to decide how the parameters for the distribution or transformation in the normalizing flows are determined using a function approximator $\varphi$, usually this boils down to deciding about the architecture of a neural network $\varphi_i$ we are using to regress parameters for each of the factors 
% $q_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i)) = q_i(x_i\,;\,\varphi_i(\text{Pa}_\mathcal{H}(x_i); \eta))$.\\
Inference amortization for universal probabilistic programming poses somewhat different challenges than for finite cardinality models, mostly because programs can yield traces (executions) with varying number of random variables.
For some of the existing approaches see work by \citet{RitchieEtAl2016} and \citet{LeEtAl2016}.

The design decisions that we elaborate on below are
(1) the factorization of the approximate posterior $q(x|y)$, 
(2) the proposal distribution for each factor $q_i$, and
(3) the function approximators.


\subsubsection*{Factorization of the approximate posterior $q(x|y)$}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \input{figures/example1/bn}
    \caption{}
    \label{fig:example1-bn}
\end{subfigure}
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \input{figures/example1/mf}
    \caption{}
    \label{fig:example1-mf}
\end{subfigure}
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \input{figures/example1/brooks}
    \caption{}
    \label{fig:example1-brooks}
\end{subfigure}
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \input{figures/example1/inverse}
    \caption{}
    \label{fig:example1-inverse}
\end{subfigure}
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \input{figures/example1/ar}
    \caption{}
    \label{fig:example1-ar}
\end{subfigure}
\caption[Simple examples]{
Graphical models for a simple generative model and different possible inverse models. 
Inverse graphical models are sorted roughly from the most approximate to the most expressive.\\
(a) A generative model,  $p(a)p(b|a)p(d|b)p(c|a)p(e|c)$; \\
(e) A mean-field inverse, $q(d)q(e)q(a|d,e)q(b|d,e)q(c|d,e)$;\\
(c) \mbox{An inverse formed with the algorithm by \citet{StuhlmullerEtAl2013}, $q(d)q(b|d)q(e)q(c|e)q(a|b,c)$;}\\
(d) An inverse formed with the NaMI algorithm,
$q(d,e)q(c|e)q(b|d,e,c)q(a|b,c)$;\\
(e) An autoregressive inverse, $q(e)q(d|e)q(c|d,e)q(b|c,d,e)q(a|b,c,d,e)$.
}
\label{fig:example1}
\end{figure}

A directed graphical model $\mathcal{G}$ defines a set of conditional independence assumptions in the model via a directed acyclic graph \citep{KollerFriedman2009}
and implies the joint probability factorizes into conditional density factors as
$p(x,y)=\prod_i p(x_i|\text{Pa}(x_i)) \prod_j p(y_j|\text{Pa}(y_j))$,
where $\text{Pa}_\mathcal{G}(x_i)$ are the parents of the node $x_i$ according to graphical model $\mathcal{G}$, see \autoref{fig:example1-bn}.

Given a graphical model of the generative model $p(x,y)$, 
we have to decide about a new graphical model for our inference network $q(x|y)$. 
% We follow the reasoning of \citet{StuhlmullerEtAl2013} and \citet{PaigeWood2016}.
% The factorization of the approximate posterior $q(x|y)$ reflects the conditional independence assumptions between random variables we impose on the approximate posterior.
The more conditional independence assumptions we impose on our approximate posterior $q(x|y)$ the easier the problem is computationally, but the less expressive $q(x|y)$ becomes and hence we make a coarser approximation.
The fewer assumption we impose the more expressive $q(x|y)$ is, but it becomes more expensive to train and use.
% Those assumptions are conveniently expressed using a framework called graphical modelling \citep{KollerFriedman2009}.

This makes us consider the spectrum of assumptions we can impose on the approximate posterior.
On one end of this spectrum lies the one of coarsest approximations we can make called the mean field assumption -- assuming that the posterior factorizes into individual factors for each random variable $x_i$ without any dependencies between them, $q(x|y)=\prod^N_iq_i(x_i\,|\,y)$, see \autoref{fig:example1-mf}.
On the other end of this spectrum lies an autoregressive factorization which makes no assumptions since any joint distribution can always be factorized according to the probability chain rule $q(x|y)=\prod^N_iq_i(x_i\,|\,x_{<i},y)$, see \autoref{fig:example1-ar}.

What motivated our work in the first part of this report was the observation by \citet{StuhlmullerEtAl2013} that
the factorization of the inference network $q(x|y)$ should be constructed such that the joint distribution defined by the original model $p(x, y)$ is identical to that of the new `inverse model` $q(x, y) = q(y)q(x|y)$, i.e. factorization of $q(x|y)$ should not impose conditional independence assumptions on the inverse model than were not present in original factorization on $p(x,y)$.
If we impose more assumptions than those present in the generative model $p(x,y)$ we can make it impossible for our approximate posterior $q(x|y)$ to match the true posterior $p(x|y)$ no matter how flexible density estimator and how powerful function approximator we use.
% In plain words the reason for this is that neural network that determines the parameters for the factor $q_i$ is not given all the necessary inputs (values of other random variables) to reconstruct the true posterior.
Even when we operate in the ASVI setting and our goal is not learning an accurate inference network but learning the model this effect is undesirable
% as limited capacity of the inference artifact forces the learnt model to adjust to this inaccurate inference hence limiting the capacity of the model itself, 
since because of the KL penalty term in the ELBO objective
$\mathcal{L}(y,q,p)=\log p(y)-KL(q(x|y) || p(x|y)) \le \log p(y)$ \citep{VAE}
optimizing under these assumptions tends to force the posterior $p_\theta(x|y)$ to satisfy the factorizing assumptions of the variational family $q$ \citep{FIVO}.

\citet{StuhlmullerEtAl2013} propose a heuristic algorithm for such `graphical model inversion` however it does introduce assumptions not present in the generative model.
To alleviate that we introduce a NaMI algorithm that uses the graphical model $\mathcal{G}$ we assume for the generative model $p$ to search for a graphical model $\mathcal{H}$ for the approximate posterior which introduces as many assumptions as possible while not introducing any assumptions not present in the $p$, and hence it does not limit the expressivity of $q$ more than what is required to be able to match the true posterior. 
Compare \autoref{fig:example1-inverse} with \autoref{fig:example1-brooks}.

We are not postulating that users should always use the factorization suggested by our method,
however we do recommend to use it as a guidance or a starting point before they decide to
introduce further assumptions into the inference network.

% Unlike mean-field and autoregressive factorizations, the factorizations produced by our method are very model-dependent and 
% we believe they provide more benefit to models with intricate graphical model structure.
% Compare VAE NaMI inverse which is just an autoregressive model with inverse of a binary tree.

% What is more -- you might wonder why we would like to limit the number of random variables on each factor of the factorization -- the goal of that is to limit the dimensionality of the conditioning set of RVs at each factor significantly.
% That translates into decreasing the dimensionality of the input to $\varphi_i$, we are analytically removing the inputs to the neural network that are unnecessary to determine the posterior.
% In case of structured models we would expect the dimensionality of the factors to be of the order $\mathcal{O}(1)$ as compared to $\mathcal{O}(N)$ for the autoregressive models where $N$ is the number of latent random variables in the model.
% We are not however circumventing one of the most significant issues of autoregressive models in general -- our lack of ability to parallelize the computation what at the moment is one of the bottlenecks in using methods such as Inverse Autoregressive Flows \citep{IAF} and Masked Autoregressive Flows \citep{MAF}, and is the motivation for the development of involved training strategies to combine the strengths of both techniques such as in \citep{ParallelWavenet}.
% However the total dimensionality of the latent variables in a model is often smaller than the that of the observed variables and in inference amortization we are only targeting the latent variables so there is still hope for sufficient efficiency for practical purposes.

% Major issue with using autoregressive NNs is that they need O(N) passes of a neural network.
% One of the issues that may arise when using NaMI inverses is a similar problem.
% Sampling from the inference network takes O(N) where N is the total dimensionality of the latent variables.
% In contrast the mean field assumption allows us to sample in O(1).
% This is also the issue with MAF.
% Maybe some kind of Parallel Wavenet approach would work here?
% NaMI inverses should take less time than autoregressive factorization but they will still be of order O(N).

% \todo{can you add figures here? think when reading the draft and design the figure}
% 
% \todo{do people add auxiliary RVs that are later marginalized like $\hat{y}$ suggested here at the bottom \url{http://pyro.ai/examples/_static/img/ss_vae_zoo.png}?}



\subsubsection*{Proposal distribution}

In order to learn a proposal for each latent random variable in the model we need to choose what distribution to use on each factor $q_i$.
At the moment the most common approach in amortized inference is to use a parametric probability distribution with the parameters set by a neural network $\varphi_i(\text{Pa}_\mathcal{H}(x_i), \eta)$.
The simplest choice one could make for the distribution of a random variable is to use the same distribution family as in the generative model.
However, in many cases this distribution might not be flexible enough to match the true posterior well.
For example, for $x \in \mathbb{R}$ a mixture of Gaussians is a more flexible choice than a single Gaussian.

An alternative to using a parametric distribution might be a recently developed family of flexible density estimators called normalizing flows \citep{RezendeMohamed2015,IAF,MAF}.
Instead of requiring the distribution to have a closed form density expression normalizing flows take a sample $z$ from some tractable distribution, commonly standard normal, and transform it using a bijective (i.e. invertible) transformation $f: \mathbb{R} \mapsto \mathbb{R}$ (or series of such transformations) into a much more complex, possibly multi-modal, random variable $x=f(z)$.
We can still evaluate the probability density at any given point $x$ since we are able to compute the determinant of the Jacobian and apply the change of variables formula between the source of randomness $z$ and the final variable $x$ according to 
$p_x(x) = p_z(z) \left|
    \mathrm{det} \frac{
      \partial f^{-1}
    }{
      \partial z\
    }
  \right|$.
To make normalizing flows feasible, function $f$ is chosen so that the determinant of the Jacobian can be easily computed (e.g. by constraining $f$ such that the Jacobian is triangular), and it is additionally parameterized by $\eta$ which is tuned such that the output random variable $x$ match the desired distribution. In practice a part of $f(z, \eta)$ is constituted by a neural network with parameters $\eta$.
% We also have to be able to take the derivative of the density with respect to the parameters $\eta$ what is required to train such a density estimator.


% In this case we are only interested in proposals over one dimensional random variables since the factorization is decided on separately by the NaMI algorithm and hence we are not subject to the inefficiency of autoregressive flows in sampling (MAF) or density evaluation of an arbitrary point that was not obtained by sampling from it (IAF).

Normalizing flows are a promising tool for learning flexible proposals for the $\mathbb{R}^d$ domain.
However, distributions in some other domains which are also relatively frequent in probabilistic modelling such as natural numbers $\mathbb{N}$, double bounded continuous $[a,b]$ or bounded continuous $\mathbb{R}^+$, are not addressed by the promise of normalizing flows and hence might require development of better density estimators for those domains.
Currently usually the proposal for $\mathbb{N}$ is Poisson, for $[a,b]$ it is Kumaraswamy or Beta, and for $\mathbb{R}^+$ it is Gamma, 
but these are not nearly as flexible as we would wish.



\subsubsection*{Function approximator}
When using either the parametric distribution or normalizing flow as the proposal we need to choose the function approximator that is used to determine the parameters of the distribution or the transformation $f$ in normalizing flow.
Usually the function appoximator is a neural network and so our decision amounts to choosing its' architecture.

The probabilistic program specifies all of the deterministic transformations applied to the samples of the random variables in the generative model
what creates an opportunity to leverage that information in designing the architecture of the neural network by attempting to invert the computation in the generative model \citep{TavaresEtAl2016,TavaresEtAl2017}.
It is yet unclear if this approach will prove of practical use in the inference compilation setting.

Another possible route to automate the selection of the architecture would be to look into the neural architecture search literature \citep{ElskenEtAl2018,ZophLe2017,pham18a}.






% Methodology
% A good starting point for an attack on that problem is to limit the range of the models we will be able to run inference amortization on. One way to limit the range of models is to limit them to the finite-cardinality graphical models and attempt to invert their structure to guide the architecture of the neural network. That line of research was started by \citet{PaigeWood2016} and serves as our starting point that we plan to build upon.


% https://arxiv.org/pdf/1610.05735.pdf use the same ordering for their guide program as for the generative model as well as the same parametric family
% they also have some sort of optimization algorithm 
% 
% Tom
% >Unfortunately, there are two key stumbling blocks that often make it difficult for this idealized view of the Bayesian machine learning approach to be realized in practice. Firstly, a process known as Bayesian inference is required to solve the specified problems. This is typically a challenging task, closely related to integration, which is often computationally intensive to solve.
% 
% In terms of settings in which amortized inference is pursued there are 3 that should be named: 
% Bayesian Networks (BN) or Graphical Models (GM), i.e. models with deterministic number of random variables (RVs); 
% Deep Generative Models (DGMs) such as VAEs,
% Universal Probabilistic Programming.



\section{Research proposal}
There are several possible extensions and improvements to the existing inference amortization frameworks and systems. 
Below I list a few research directions that I am in good position to pursue.
The resulting projects vary in size, but each could yield a workshop or conference publication.
Some of them might end up being combined in a single paper.
I will provide more thorough description of each one below.
The list consists of the following themes and projects\\
\emph{Extensions to the inference compilation framework}\\
\textbf{A} \quad Amortized Monte Carlo Integration\\
\emph{Improvements and automation of inference compilation}\\
\textbf{B} \quad Normalizing flows as proposals for inference compilation\\
\textbf{C} \quad Choosing an inference network factorization in a data-driven way\\
% \textbf{E} \quad Making use of the information about the deterministic computations in the Bayesian networks and probabilistic programs\\
% \textbf{F} \quad Better ways of sampling from the model for the purpose of inference amortization\\
\emph{Applications of amortized inference in engineering, medicine and physical sciences}\\
\textbf{D} \quad Applications showcasing benefits of NaMI for models with rich structure\\




\subsubsection*{A \quad Amortized Monte Carlo Integration}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/diagram.pdf}
  \caption{
  Schematic diagram of the Amortized Monte Carlo Inference framework.
  The black elements are parts of the classic inference compilation framework,
  the teal elements are the new elements of the pipeline introduced by AMCI.
  The dashed line refers to the opportunity of at least partial specification of the neural network architecture based on the probabilistic model program supplied by the user, as outlined in \autoref{sec:design}. 
  Original diagram courtesy of \citet{LeEtAl2016}.}
  \label{fig:amci}
\end{figure}
I am currently working on project A, Amortized Monte Carlo Integration (AMCI), which is an extension to inference compilation in settings when we have knowledge about the functions $f(x)$ we would like to take expectations of with respect to the posterior $\E_{p(x|y)}[f(x)]$.
Having that information ahead of the time of the training of the system allows us to learn a more efficient importance sampling proposal along with the approximation to the posterior.
To fully utilize this amortization setup we formulate a novel estimator which is a combination of two non-self-normalized importance sampling estimates which entertains zero variance when optimal proposals are used for each of the individual estimators.

To account for cases in which multiple possible target functions may be of interest, AMCI also allows for amortization over parametrized functions.
If the target function can be parameterized as $f(x;\theta)$, then by extending our target distribution with a pseudo prior $p(\theta)$ we are able to amortize over possible target functions as well.
The choice of $p(\theta)$ determines how much importance we assign to different possible functions that we would like to amortize over.

See \autoref{fig:amci} for the overview of the framework and
the manuscript attached at the end of this report for the technical details.

We have already published preliminary results at UAI 2018 workshop on Uncertainty in Deep Learning and at International Conference on Probabilistic Programming 2018 (without proceedings).
Current plan is to perform additional experiments to showcase the utility of the method, 
to establish a process to tune the hyperparameters and
to submit the work to AIStats 2018, 
and in case of rejection to resubmit it to ICML 2019.
 
% Since this project is in further stage of the development more details about it will follow in the next section.


% \subsubsection*{B \quad Probabilistic programming semantics for integration}
% Project A naturally leads to project B, establishing semantics for performing integration in probabilistic programming systems.
% % We present first thoughts and suggestions in \citep{golinski2018probprog}.
% % The semantics we propose are based on the semantics and program transformations introduced by \citet{rainforth2016bopp}.
% Specifically, we consider 
% how one might support it in Anglican~\citep{anglican}.
% Based on the query macro \defopt of~\citep{rainforth2016bopp}, 
% we introduce \clj{defint}.  Like \defopt, \clj{defint} adjusts Anglican's
% standard query macro, \defquery, by providing an series of symbols as an additional input.
% The role of this input is to identify the variables in the program which
% are function parameters $\theta$.  The value of the 
% function $f(x;\theta)$ is then given by the output of the query.
% We can thus express our previous
% example as follows
% \begin{lstlisting}[basicstyle=\ttfamily\small,frame=none]
% (defint example [y] [$\theta$]
%  (let [x (sample (normal 0 1))
%        $\theta$ (sample (uniform-continuous -5 5))]
%   (observe (normal x 1) y)
%   (> x $\theta$)))
% \end{lstlisting}
% The conditional distribution of this query corresponds to the distribution
% $p(x|y)p(\theta)$, while its output represents $f(x;\theta)$.  It thus
% contains all the information we require to carry out AMCI by estimating
% gradients of $\mathcal{J}$ and $\mathcal{J}'$:
% we can sample from $p(x,y)p(\theta)$ and have evaluations of
% $f(x;\theta)$ at these points.  Therefore by coupling the AMCI approach
% with the inference compilation approach introduced for Anglican
% in~\cite{LeEtAl2016}, we have everything required for
% training an ``integration compilation'' artifact.
% 
% The syntax used to identify which variables correspond to
% $\theta$ is necessary because
% a) we do not need to learn a proposal for $\theta$ and b) at test time, we need
% a way to fix the value of $\theta$.  The latter of this is simply dealt
% with using the maximum marginal likelihood transformation applied for \defopt
% in~\cite{rainforth2017thesis}.   This effectively removes the
% \sample statements for $\theta$, replacing them with identity functions
% and making $\theta$ an input to the program.  With this transformation and
% the appropriately adapted trained amortized proposals, we thus
% have a means of automatically applying AMCI at test time.


\subsubsection*{B \quad Normalizing flows as proposals for inference compilation}
Project C revolves around using normalizing flows as more flexible proposal distributions for inference compilation, as described in \autoref{sec:design}.
% The most prevalent and challenging type of random variable to formulate a proposal for is a continuous variable with an unbounded support.
Current practice for the continuous random variables with unbounded support is to use a multimodal distribution such as Mixture of Gaussians with a fixed number of components $K$.
This choice may fail in several ways -- 
(1) there is no way for the proposal to match the true posterior if the posterior contains more than $K$ modes,
(2) the individual modes might not be very well approximated by a Gaussian, e.g. when they are highly asymmetric,
(3) the tails of the distribution might be heavier than those of Gaussian distribution.
Any of those scenarios might easily lead to a large (or even infinite) variance of the final estimator, 
and hence to dramatic decrease in its sample efficiency.
An increasingly viable alternative to this approach is the growing family of flexible neural density estimation methods based on the idea of normalizing flow \citep{RezendeMohamed2015}.
I am planning to evaluate the utility of these methods, especially in the context of the AMCI framework. 
% Currently the major difficulty of using normalizing flows for this purpose lies in the fact that methods allow either fast probability density evaluation at some data point $y$ (which is necessary for training) or the ability to efficiently sample (which is necessary at runtime).
% There are some attempts to circumvent this issue such as ParallelWavenet \citep{ParallelWavenet}, but anecdotal evidence suggests that approach is difficult to tune and train.
% Although I do not know of any successful attack on this problem yet I still deem it worthy to evaluate the utility of neural density estimators for amortized inference and its effect on sample efficiency of the final estimates even if we are not yet able to realize wall-time efficiency improvements in this way yet.

\subsubsection*{C \quad Choosing an inference network factorization in a data-driven way}
This project is a potential extension to the NaMI method.
At the moment NaMI algorithm is forced to choose among multiple plausible faithful inverses of the generative graphical model and we use  heuristics to choose decide between those options.
% At the moment NaMI computes topological or reverse-topological sort orders, solves the draws between different orderings according to min-fill heuristic (i.e. minimizes the size of the cliques in the graph), computes two inverses and suggests to evaluate the performance of both.
My idea is to extend our current method to pick the factorization in a data-driven rather than a heuristic way.
At a high level the new method would draw samples from the generative model and then use kernel density estimation to examine the form of the factors under different plausible factorizations. 
The `simpler` the forms of the factors are the easier they are to target with proposal distributions.
If a suitable measure of simplicity of a factor distribution is established it might be possible to further automate the process of choosing a suitable inference network factorization.


\subsubsection*{D \quad Applications showcasing the benefits of NaMI for models with rich structure}
At the moment we are lacking compelling examples showcasing the benefits of using the NaMI method.
We expect that our method yields most benefits for models with rich structure, and such can be found in the domains where probabilistic modelling is a prevalent tool.
My plan is to seek suitable applications by looking for collaborators in engineering, finance, medicine or physical sciences.


\newpage
\subsection{Preliminary schedule}
The following is a rough schedule for the rest of my DPhil degree, the milestones are pieces of work I intend to complete for the relevant machine learning conferences.

\begin{table}[h!]
\begin{tabular}{lp{11cm}}
Present -- October 2019         & Further work on projects A\&B, submission to AIStats \\
October 2018 -- February 2019   & In case of rejection or delay further work on A\&B for submission to ICML, or investigation of project C \\
February 2019 -- May 2019       & Work on C or/and D depending on which one holds more promise, submission to NIPS or partial work to an ICML workshop \\
May 2019 -- May 2020            & Reevaluation of available avenues for research and picking a new project \\
June 2020 -- October 2020       & Industrial internship \\
October 2020 -- December 2020   & Writing up thesis
\end{tabular}
\end{table}

\newpage 
\bibliographystyle{plainnatnourl}
\bibliography{bibliography/bibliography} 

\includepdf[pages=-]{amci}

% \includepdf[pages=-]{supplementary}

\end{document}
