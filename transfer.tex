\documentclass[12pt]{article}
\input{packages-and-defs}

\begin{document}
\input{title}

\includepdf[pages=-]{nami}

\title{Improving, Extending and Automating Inference Amortization for First Order Probabilistic Programs}
\date{}
\maketitle

\vspace{-90pt}

\section{Introduction}

In this part of the report, I introduce the background knowledge, review the recent literature around amortized inference and present future directions for this avenue of research. 
I also present a preliminary schedule for the rest of my DPhil programme.

\subsection{Automating inference amortization for first order probabilistic programs}
\todo{describe the overall theme of the work}

\todo{introduce what first order probabilstic programs are clearly}


\section{Background and literature review}

\subsection{Probabilistic machine learning}
Probabilistic machine learning is a branch of machine learning which makes use of probabilistic modelling framework to reason about learning models and for incorporating uncertainty into that process \citep{Ghahramani2015}.
Its major advantages are the ability to incorporate prior knowledge by make explicit modelling assumptions, ability to obtain well calibrated uncertainties and interpretability of the models that follows.
The price we pay is often computationally expensive process of learning as well as the risk of model misspecification by making inaccurate assumptions what results in learning inaccurate models.

Learning in probabilistic models boils down to the process called inference - determining the probability distribution of a quantity/information of interest based the prior belief and the data - which is the core problem in probabilistic machine learning.
In the mathematical terms - our probabilistic model defines a joint distribution $p(x,y)$ over the hidden (or latent) $x$ and the observed random variables (RVs) $y$.
The prior beliefs are encoded in the model as so called prior $p(x)$.
The focal point of probabilistic inference is the posterior $p(x|y)$ - the probability distribution over the hidden RVs $x$ given the observed value of $y$.
Bayes' Law, easily the most recognizable equation in bayesian statistics, dictates how to obtain the posterior $p(x|y) = p(x,y)/p(y)$.
The challenging part of the entirety of probabilistic inference is estimating the so-called evidence $p(y)$ what requires performing or approximating the integration $\int p(x,y) \text{d}x$.

This is however not the end of the story - 
what we are actually interested in is the probability distribution over some quantity of interest which is usually a function of the latent variables $g=f(x)$, the probability distribution of interest may hence be expressed as $p(g)$.
Often we only seek a point estimate, i.e. the expected value of that function under the posterior $\mu_g=\E_{g \sim p(g)}[g]=\E_{x \sim p(x|y)}[f(x)]$ what might be arbitrarily difficult.

We do have efficient inference methods for certain classes of models, however we lack general purpose inference methods that would be efficient for all the models and that is what often stops us from formulating and attempting to use more complex probabilistic 
models\footnote{In many of those cases inefficient inference implies that inference is infeasible at the scale we are interested in.}.

One of the the major advantages of the probabilistic framework is the interpretability it offers.
It is a sought after characteristic, especially in the highly regulated domains such as medicine, jurisprudence, finance and autonomous operation of robots, most notably - autonomous driving.

\subsection{Probabilistic programming}
Probabilistic programming is an effort which focuses on creating, or modifying existing, programming languages to allow the users to define probabilistic models in a flexible manner and run inference on them.
The goal is to abstract away the process of inference and disentangle the problems of modelling and inference.
The model is specified by the user while the inference should be performed in automated way by the probabilistic programming backend inference engine with minimal input from the user.
The motivation behind this approach is to allow users with deep domain knowledge, but without the expertise in inference, to use the probabilistic modelling tools.

\todo{modularity}

To achieve that we need general purpose inference algorithms.
However even general purpose inference algorithm do not behave the same way on all models - their efficiency varies, some are better suited to particular class of models than others.
An ideal probabilistic programming system should be able to automatically select the most efficient of the inference methods in its arsenal for the given problem, including not only all of the general purpose algorithms but also more bespoke ones.
An example of such attempts is work by \citet{ZinkovEM} where they present a method to perform Expectation Maximization \citep{EM} if closed-form solutions can be statically discovered by the compiler.
\todo{ask Rob for comments}

There is a trade-off between the user's freedom of choosing the model and the efficiency of inference.
This leads to a spectrum of probabilistic programming systems - some of them limit the expressivity of the model specification language by appropriate design of language semantics to allow the use of an inference method particularly effective for this constrained class of models, while others strive to provide unconstrained expressivity at the cost of inference performance.
For this work the most relevant distinction of this kind is into the systems that focus on models equivalent in expressivity to Bayesian Networks (Bayes Nets, BNs), such as Stan \citep{Stan}, BUGS \citep{WinBUGS,BUGSproject} and its cousin JAGS \citep{JAGS}, or to Graphical Models (GMs) such as Infer.NET \citep{InferNET}, and so-called `universal probabilistic programming languages` built on top of existing Turing-complete programming languages such as Anglican \citep{anglican}, Church \citep{GoodmanEtAl2008} and Venture \citep{venture}.
The last category is also known as higher-order probabilistic programming languages because they allow stochastic recursion, as contrasted to first-order probabilistic programming languages which are limited to graphical models with finite and deterministic number of random variables.

There is also an upsurge in development of so-called deep probabilistic programming languages such as Edward \citep{TranEtAl2016}, Pyro \citep{Pyro2018} and Probtorch \citep{Siddharth2017} which main focus is on stochastic variational inference for deep generative models and hence their expressivity is limited to Bayesian Networks.
\todo{this might be wrong, what is actually the class of models for those languages?}
% \todo{why there is so little in probabilistic programming for MRFs/undirected GMs? Infer.NET}

% Some say that users should not be cheated \footnote{Personal conversations with Michael Osborne}.

\todo{describe what first order probabilistic program is}

\subsection{Inference in probabilistic programming systems}
Inference in probabilistic programming systems roughly divide into two broad categories: variational or Monte Carlo based methods.
With a healthy dose of generalization - variational methods are generally not guaranteed to converge to the true posterior, but are relatively quick, whereas Monte Carlo methods are generally guaranteed to converge to the true posterior in the infinite time limit, but there is no guarantee on the rate of convergence to the posterior and the inference is usually slow.
What is more in both cases inference needs to be started again from scratch whenever new set of observed variables (i.e. dataset) $y$ is obtained and we want to obtain a posterior for that dataset.  

Many of the probabilistic programming systems with limited expressivity of the models does this to make the models amenable to particular classes of inference algorithms. 
Best examples are STAN which expressivity is limited to Bayesian Networks with no discrete variables to be able to run use No-U-Turn-Sampler (NUTS) \citep{NUTS}, an improved version of Hamiltonian Monte Carlo (HMC) \citep{HMC}, or Infer.NET which is limited to graphical models to be able to use variational methods such as variational message passing \citep{variationalmessagepassing} and expectation propagation \citep{EP} algorithms.

\todo{rewrite the paragraph below}
\todo{not exactness but if we want to be able to reason or quantify our error, unlike variational methods where we simply cannot bound or estimate the error}
If we are after inference with guarantees of exactness we have to resort to Monte Carlo methods.
However there exist applications for which MC methods are not fast enough to run inference at the required pace, in some cases close to real-time.
Examples of such domains include finance, autonomous robot control and real-time medical diagnostics.
In all of them well-calibrated uncertainty quantification is of crucial importance.
Inference amortization is one of the possible solutions for this problem.


\subsection{Inference amortization}

\todo{introduce inference compilation as a term for the second variant of inference amortization}

\todo{use those terms $\varphi(y)$ etc to establish a consistent mathematical narrative}

Typically, this amortization
artifact takes the form of a parametrized proposal, $q(x ; \varphi(y))$, which takes
in data $y$ and regresses these to proposal parameters $\varphi(y)$, generally using
a deep neural network.
For consistency with the literature and to avoid clutter, we will often
use the shorthand $q(x|y)$ to represent this \emph{inference network}.
Though the exact process varies with context,
the inference network is usually trained either by drawing latent-data
sample pairs from a fixed joint distribution
$p(x,y)$~\citep{ritchie2016deep,PaigeWood2016,LeEtAl2016}, or 
as part of a stochastic variational inference scheme~\citep{HoffmanEtAl2013,VAE,RezendeEtAl2014},
our work focuses mostly on the former of the two settings.
Once trained, it provides an efficient means of approximately
sampling from the posterior of a particular dataset, e.g. using importance sampling.

This approach allows us to utilize the high run-time performance of neural networks within the probabilistic modelling framework. 
Importantly we are not sacrificing the interpretability offered by the probabilistic framework.



\section{Design of inference amortization}

\todo{somehow clear the confusion between inference amortization in general and inference compilation setting, describe that pretty much all of the decisions we have to make in the context of inference amortization for joint model&inference learning are the same but the difference is that we do really get the advantage to use the structure described by the probabilistic program what will become evident later on in the description}

\todo{modify this figure to only describe existing inference compilation}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/diagram.pdf}
  \caption{
  Schematic diagram of inference compilation for probabilistic programs. 
  % Black elements refer to 
  Original diagram courtesy of \citet{LeEtAl2016}.}
  \label{fig:inf-amortization}
\end{figure}
\todo{describe the figure and tie it into the narrative}
\todo{overall my research agenda is to continue contributing to the automation of inference amortization along those 3 questions and others like AMCI}

Inference amortization is a method which allows to significantly accelerate run-time inference, wherein one looks to ``compile away'' the cost of inference
across different possible datasets
by learning an artifact that can be used to assist the inference process
at run time for a given dataset
\citep{StuhlmullerEtAl2013, VAE, ritchie2016deep, PaigeWood2016, LeEtAl2016, LeEtAl2017, FIVO, NaessethEtAl2017}.

Implementing inference amortization for a given problem requires several design decisions that we elaborate on below:\\
1. The factorization of the approximate posterior $q(x|y)$: deciding on the graphical model $\mathcal{H}$ structure of the approximate posterior what implies it factorizes as $q(x|y)=\prod^N_iq_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i))$, this is equivalent to choosing the variational family\\
\todo{can I call this variational family?}
2. The density estimator: typically we have to decide on a parametric distribution for each one of the factors $q_i$, but we could use more flexible density estimators that do not constrain us to parametric distributions, e.g. normalizing flows \citep{RezendeMohamed2015} \\
\todo{is there a better way to phrase this? e.g. MADE is considered a density estimator, but you still have to decide about the parametric distribution at the end, but in case of normalizing flows we don't}
3. The function approximator: we need to decide how the parameters for the distribution or transformation in the density estimator above are determined using a function approximator, usually this boils down to deciding about the architecture of a neural network we are using for each factor $q_i$\\

% In terms of automating the guide program generation there are multiple choices to be made and hence multiple aspects to be automated, going from the top to the bottom:\\
% 1. the variational family (mean-field, autoregressive or more structured conditional independence assumptions for the inference network i.e. the graphical model for the inference network)\\
% 2. neural network architecture for each of the factors in the inference network\\
% 3. the parametric distribution or a different type of neural density estimator\\



\subsubsection*{Factorization of the approximate posterior $q(x|y)$}
The factorization of the approximate posterior $q(x|y)$ reflect the conditional independence assumptions between random variables we impose on the approximate posterior.
The more assumptions we impose the easier the problem is computationally, but the less expressive our approximate posterior $q(x|y)$ becomes and hence we make a coarser approximation.
The fewer assumption we impose the more expressive our approximate posterior $q(x|y)$ is, but it becomes more expensive to train and use.

This creates a spectrum of the assumptions we can impose on the problem.
On one end of this spectrum lies the one of coarsest approximations we can make called the mean field assumption - assuming that the posterior factorizes into individual factors for each random variable $x_i$ without any dependencies between them, $q(x|y)=\prod^N_iq_i(x_i\,|\,y)$.
On the other end of this spectrum lies an autoregressive factorization which makes no assumptions since any joint distribution can always be factorized according to the probability chain rule $q(x|y)=\prod^N_iq_i(x_i\,|\,x_{<i},y)$.

Our observation which motivated the work in the first part of this report \citep{Webb2018} was that if we impose more assumptions than those present in the generative model $p(x,y)$ we can make it impossible for our approximate posterior $q(x|y)$ to match the true posterior $p(x|y)$ no matter how flexible density estimator and how powerful function approximator we use.
Even when our goal is not learning an accurate inference artifact but learning the model this effect is undesirable
% as limited capacity of the inference artifact forces the learnt model to adjust to this inaccurate inference hence limiting the capacity of the model itself, 
since because of the KL penalty term in the ELBO objective
$\mathcal{L}(y,q,p)=\log p(y)-KL(q(x|y) || p(x|y)) \le \log p(y)$ \citep{VAE}, 
optimizing under these assumptions tends to force $p$â€™s posterior to satisfy the factorizing assumptions of the variational family \citep{FIVO}.

That observation and issues that follow from it prompted us to make use of the set of conditional independence assumptions in the generative model expressed in the form of a graphical model $\mathcal{G}$ to search for a maximal set of assumptions for the approximate posterior which would not introduce any assumptions not present in the generative model, and hence not limit the expressivity of $q$ any more than what is required to match the true posterior, but at the same time introducing as many assumptions as possible to make it less computationally expensive.
This way our method allows us to identify the intermediate point on the scale introduced above which is chosen so that we introduce maximum number of assumptions without limiting the expressivity of the approximate posterior $q$ beyond what's required to capture the true posterior as dictated by the assumptions made in the generative model $p$.
\todo{add some information how inverses were derived in the past}

For the details of our method that we named NaMI for Natural Minimal I-map please see the first part of the report. 
With a tiny bit of simplification - it takes a set of conditional independence assumptions in the generative model expressed in the form of a graphical model $\mathcal{G}$ as input and produces a set of assumptions for the approximate posterior in the form of a graphical model $\mathcal{H}$ as output.
Once we have obtained $\mathcal{H}$, the next step is to use it to learn an inference network, $q_\psi(x|y)$, where $\psi$ are the parameters of the neural networks constituting the full inference network. 
For this, we use the factorization given by $\mathcal{H}$, that is, $q_\psi(x|y)=\prod^N_i q_i(x_i\,|\,\text{Pa}_\mathcal{H}(x_i))$, where $\text{Pa}_\mathcal{H}(x_i)$ stands for parents of the node $x_i$ according to graph $\mathcal{H}$.
For each factor $q_i(x_i\,|\,\text{Pa}(x_i))$, we must decide both the class of distributions for $x_i\,|\,\text{Pa}(x_i)$, and how the parameters for that class are calculated.
Once learned, we can both sample from, and evaluate the density of, the inference network for a given dataset by considering each factor in turn.

We are not claiming that people should always use the factorization NaMI is suggesting, people are welcome to introduce further assumptions to their factorizations however we suggest to use NaMI to get an idea of what constitutes a baseline factorization that allows to recover the posterior.

What is more - you might wonder why we would like to limit the number of random variables on each factor of the factorization - the goal of that is to limit the dimensionality of the problem at each factor significantly.
In case of structured models we would expect the dimensionality of the factors to be of the order $\mathcal{O}(1)$ as compared to $\mathcal{O}(N)$ for the autoregressive models where $N$ is the number of latent random variables in the model.
We are not however circumventing one of the most significant issues of autoregressive models in general - our lack of ability to parallelize the computation what at the moment is one of the bottlenecks in using methods such as Inverse Autoregressive Flows \citep{IAF} and Masked Autoregressive Flows \citep{MAF}, and is the motivation for the development of involved training strategies to combine the strengths of both techniques such as in \citep{ParallelWavenet}.
However the total dimensionality of the latent variables in a model is often smaller than the that of the observed variables and in inference amortization we are only targeting the latent variables so there is still hope for sufficient efficiency for practical purposes.

\todo{can you add figures here? think when reading the draft and design the figure}

\todo{do people add auxiliary RVs that are later marginalized like $\hat{y}$ suggested here at the bottom \url{http://pyro.ai/examples/_static/img/ss_vae_zoo.png}?}



\subsubsection*{Density estimator}
To learn a distribution for each of the factors $q_i$ we need to use density estimators, a individual one for each of the factors.
At the moment the most common approach in amortized inference is to use a parametric probability distribution with the parameters set by a neural network $\varphi(y)$, as the density estimator.
The simplest, most of the time quite naive, choice one could make for the distribution of a factor is to use the same distribution family as in the generative model for a given random variable.
In many cases this family might not be flexible enough to match the true posterior well.
For the domain of $\mathbb{R}^d$ a mixture of gaussians is a more flexible choice than a single gaussian.

Making these choices however is difficult and quite ad-hoc, and often the true posterior simply does not match any parametric distribution.
A relatively recent, promising and flexible density estimator that could help us with this task are normalizing flows first introduced in \citet{RezendeMohamed2015}.
Instead requiring the distribution to have a closed form density expression normalizing flows take a sample $z$ from some tractable distribution, commonly standard normal, and transform it using a bijective (i.e. invertible) transformation $f: \mathbb{R} \mapsto \mathbb{R}$ (or series of such transformations) into a much more complex, possibly multi-modal, random variable $x=f(z)$.
We can still evaluate the probability density at any given point $x$ since we are able to compute the determinant of the Jacobian and apply the change of variables formula between the source of randomness $z$ and the final variable $x$ according to 
$p_x(x) = p_z(z) \left|
    \mathrm{det} \frac{
      \partial f^{-1}
    }{
      \partial z\
    }
  \right|
  = p_z(z) \left|
    \mathrm{det} \frac{
      \partial f
    }{
      \partial z\
    }
  \right| ^{-1}$.
To make normalizing flows feasible function $f$ is chosen so that the determinant of the Jacobian can be easily computed (e.g. by constraining $f$ such that the Jacobian is triangular), and it is additionally parameterized by $\phi$ which are tuned such that the output random variable $x$ match the desired distribution. In practice a part of $f(z, \phi)$ is constituted by a  neural network and parameters $\phi$ belong to that neural network \citep{IAF,MAF}.
We can also take the derivative of the density with respect to the parameters $\phi$ what is required to train such a density estimator.
Normalizing flows are thus a promising tool for dealing with the lack of flexibility in cases where we are looking for proposals for the $\mathbb{R}^d$ domain.

In this case we are only interested in proposals over one dimensional random variables since the factorization is decided on separately by the NaMI algorithm and hence we are not subject to the inefficiency of autoregressive flows in sampling (MAF) or density evaluation of an arbitrary point that was not obtained by sampling from it (IAF).

However, distributions in some other domains which are also relatively frequent in probabilistic modelling such as natural numbers $\mathbb{N}$, double bounded continuous $[a,b]$ or bounded continuous $\mathbb{R}^+$, are not addressed by the promise of normalizing flows and hence might require development of better density estimators for those domains.
Currently they are often addressed by the following parametric distributions, respectively: Poisson, Kumaraswamy or Beta, and Gamma.

\todo{can you add figures here? think when reading the draft and design the figure}



\subsubsection*{Function approximator}
When using either of the methods for density estimation in the previous section we need to decide about the function approximator that is used to determine the parameters of probability distribution or the transformation $f$ in normalizing flow.
These days that amounts to choosing the architecture of a neural network.

\todo{keep those headlines?}

\emph{Deterministic structure inversion}\\
Since in a probabilistic program we are given the details of all the deterministic transformations applied to the samples of the random variables in the model there is hope that we can make use of that information to automatically design, or at least guide the process of designing, the neural network.   
Efforts on that front are being pursued in \citep{TavaresEtAl2016,TavaresEtAl2017} and we believe they might hold some promise.

\emph{Neural architecture search}\\
Another, somewhat less inventive, route to automate the selection of the architecture would be to look into the neural architecture search literature \citep{ElskenEtAl2018,ZophLe2017,pham18a}.

\todo{can you add figures here? think when reading the draft and design the figure}






% Methodology
% A good starting point for an attack on that problem is to limit the range of the models we will be able to run inference amortization on. One way to limit the range of models is to limit them to the finite-cardinality graphical models and attempt to invert their structure to guide the architecture of the neural network. That line of research was started by \citet{PaigeWood2016} and serves as our starting point that we plan to build upon.


% https://arxiv.org/pdf/1610.05735.pdf use the same ordering for their guide program as for the generative model as well as the same parametric family
% they also have some sort of optimization algorithm 
% 
% Tom
% >Unfortunately, there are two key stumbling blocks that often make it difficult for this idealized view of the Bayesian machine learning approach to be realized in practice. Firstly, a process known as Bayesian inference is required to solve the specified problems. This is typically a challenging task, closely related to integration, which is often computationally intensive to solve.
% 
% In terms of settings in which amortized inference is pursued there are 3 that should be named: 
% Bayesian Networks (BN) or Graphical Models (GM), i.e. models with deterministic number of random variables (RVs); 
% Deep Generative Models (DGMs) such as VAEs,
% Universal Probabilistic Programming.


\section{Research proposal}
\todo{is it all right if I use first person in the research proposal?}
There are several possible extensions and improvements to the existing inference amortization frameworks and systems. 
Below I list a few research directions that I am in good position to pursue.
They vary in size, but each could yield a workshop or conference publication, some of them might end up being combined in a single paper for the purpose of publication.
I will provide more thorough description of each below.
The list consists of the following projects\\
\textbf{A} \quad Amortized Monte Carlo Integration\\
\textbf{B} \quad probabilistic programming semantics for integration\\
\textbf{C} \quad utility of neural density estimators for amortized inference\\
\textbf{D} \quad picking an order of variable elimination for NaMI method in a data-driven, model-specific way in the inference compilation setting\\
\textbf{E} \quad applications of amortized inference in engineering, medicine and physical sciences\\
\textbf{F} \quad better ways of sampling from the model for the purpose of inference amortization\\
\textbf{G} \quad making use of the information about the deterministic computations in the Bayesian networks and probabilistic programs\\


\subsubsection*{A \quad Amortized Monte Carlo Integration}
I am currently working on project A, Amortized Monte Carlo Integration. 
We already have preliminary results published at UAI workshop on Uncertainty in Deep Learning \citep{golinski2018uai} and at International Conference on Probabilistic Programming \citep{golinski2018probprog}. 
Since this project is in further stage of the development more details about it will follow in the next section.


\subsubsection*{B \quad Probabilistic programming semantics for integration}
Project A naturally leads to project B, establishing semantics for performing integration in probabilistic programming systems.
% We present first thoughts and suggestions in \citep{golinski2018probprog}.
% The semantics we propose are based on the semantics and program transformations introduced by \citet{rainforth2016bopp}.
Specifically, we consider 
how one might support it in Anglican~\citep{anglican}.
Based on the query macro \defopt of~\citep{rainforth2016bopp}, 
we introduce \clj{defint}.  Like \defopt, \clj{defint} adjusts Anglican's
standard query macro, \defquery, by providing an series of symbols as an additional input.
The role of this input is to identify the variables in the program which
are function parameters $\theta$.  The value of the 
function $f(x;\theta)$ is then given by the output of the query.
We can thus express our previous
example as follows
\begin{lstlisting}[basicstyle=\ttfamily\small,frame=none]
(defint example [y] [$\theta$]
 (let [x (sample (normal 0 1))
       $\theta$ (sample (uniform-continuous -5 5))]
  (observe (normal x 1) y)
  (> x $\theta$)))
\end{lstlisting}
The conditional distribution of this query corresponds to the distribution
$p(x|y)p(\theta)$, while its output represents $f(x;\theta)$.  It thus
contains all the information we require to carry out AMCI by estimating
gradients of $\mathcal{J}$ and $\mathcal{J}'$:
we can sample from $p(x,y)p(\theta)$ and have evaluations of
$f(x;\theta)$ at these points.  Therefore by coupling the AMCI approach
with the inference compilation approach introduced for Anglican
in~\cite{LeEtAl2016}, we have everything required for
training an ``integration compilation'' artifact.

The syntax used to identify which variables correspond to
$\theta$ is necessary because
a) we do not need to learn a proposal for $\theta$ and b) at test time, we need
a way to fix the value of $\theta$.  The latter of this is simply dealt
with using the maximum marginal likelihood transformation applied for \defopt
in~\cite{rainforth2017thesis}.   This effectively removes the
\sample statements for $\theta$, replacing them with identity functions
and making $\theta$ an input to the program.  With this transformation and
the appropriately adapted trained amortized proposals, we thus
have a means of automatically applying AMCI at test time.


\subsubsection*{C \quad Utility of neural density estimators for amortized inference}
Project C revolves around using more flexible density estimators for the proposal distributions amortized inference produces.
The most prevalent and challenging type of random variable to formulate a proposal for is a continuous variable with an unbounded support.
Current practice is to use a flexible, multimodal distribution such as Mixture of Gaussians with a fixed number of components $K$.
This choice may fail in several ways - 
(1) there is no way for the proposal to match the true posterior if the posterior contains more than $K$ modes,
(2) the individual modes might not be very well approximated by a Gaussian, e.g. when they are highly asymmetric,
(3) the tails of the distribution might be heavier than those of Gaussian distribution.
Any of those scenarios might easily lead to a large (or even infinite) variance of our estimator, and hence to decrease in its sample efficiency.
An increasingly viable alternative to this approach is the growing family of flexible neural density estimation methods based on the idea of normalizing flow \citep{RezendeMohamed2015}.
To the best of our knowledge so far there were few attempts to utilize those in the context of inference amortization.
Currently the major difficulty of using normalizing flows for this purpose lies in the fact that methods allow either fast probability density evaluation at some data point $y$ (which is necessary for training) or the ability to efficiently sample (which is necessary at runtime).
There are some attempts to circumvent this issue such as ParallelWavenet \citep{ParallelWavenet}, but anecdotal evidence suggests that approach is difficult to tune and train.
Although I do not know of any successful attack on this problem yet I still deem it worthy to evaluate the utility of neural density estimators for amortized inference and its effect on sample efficiency of the final estimates even if we are not yet able to realize wall-time efficiency improvements in this way yet.

\subsubsection*{D \quad Data-driven variable elimination ordering for NaMI}
Project D is a potential extension to the Natural Minimal Inverse (NaMI) method of inverting graphical models introduced by us in \citep{golinski2018uai}.
NaMI algorithm is forced to choose among multiple possible orderings of variable elimination, each of them yielding a minimal faithful inverse.
At the moment NaMI computes topological or reverse-topological sort orders, solves the draws between different orderings according to min-fill heuristic (i.e. minimizes the size of the cliques in the graph), computes two inverses and suggests to evaluate the performance of both.
My idea is to look at the shape of the 


\subsubsection*{E \quad Better applications for NaMI}
Project E would focus on finding more convincing and applied models and examples to showcase the utility of the NaMI method, it can most probably be combined  


\subsubsection*{F \quad Better methods of sampling for inference amortization}
This would involve looking into how much data neural networks require to learn part of the domain.


\subsubsection*{G \quad Deterministic inversion of first order probabilistic programs}
making use of the information about the deterministic computations in the Bayesian networks and probabilistic programs\\




\subsection{Preliminary schedule}

The following is a rough schedule for the rest of my DPhil degree, the milestones are pieces of work I intend to complete for relevant machine learning conferences.

\begin{table}[h!]
\begin{tabular}{lp{11cm}}
Present - October 2019         & Further work on projects ABC, possible submission to AIStats \\
October 2018 - February 2019   & In case of rejection or delay further work on ABC for submission to ICML, or investigation of projects D and/or E \\
February 2019 - May 2019       & Work on C or/and D depending on which one holds more promise (it might be feasible to execute both of them at the same time), submission of full work to NIPS or partial work to ICML workshop \\
May 2019 - May 2020            & Reevaluation of available avenues for research and a new project \\
June 2020 - October 2020       & Internship \\
October 2020 - December 2020   & Writing up thesis
\end{tabular}
\end{table}





\newpage 
\bibliographystyle{plainnat}
\bibliography{bibliography/bibliography} 

\end{document}
