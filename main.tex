\documentclass[12pt]{article}
\input{packages-and-defs}

\begin{document}
\input{title}

% \title{Improving, Extending and Automating\\Amortized Probabilistic Inference and Data Compression}
\title{Advances in Probabilistic Inference and Data Compression using Deep Learning}
\date{}
\maketitle

\vspace{-50pt}


\section{Introduction}

This report consists of three parts:
\begin{itemize}
\item Description of the projects I worked on and I intend to use for my thesis,
\item Planned steps to completion of my thesis,
\item The manuscript of my paper \emph{Amortized Monte Carlo Integration} published at ICML 2019.
\end{itemize}



\section{Progress towards the thesis}

Outside of the manuscript attached in the last part of this report,
I have been working on several other projects -- 
some of them have been completed, some are in progress (in varying stage of completion), 
and some of them are only at the point of exploration.

List of my projects and resulting publications, chronologically:
\begin{enumerate}
\item \emph{Faithful Inversion of Generative Models for Effective Amortized Inference}, second author, published at NeurIPS 2018. \citep{Webb2018faithful}
\item \emph{Amortized Monte Carlo Integration}, shared first-authorship, published at ICML 2019, Best Paper Runner Up award. \citep{Golinski2019amci}
\item \emph{Improving Normalizing Flows via Better Orthogonal Parameterizations}, shared first-authorship, published at ICML 2019 Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models; work in progress to extend to a conference publication. \citep{Golinski2019orthogonal}
\item \emph{Target–Aware Bayesian Inference: How to Beat Optimal Conventional Estimators}, shared first-authorship, published in JMLR 2020. \citep{Rainforth2020tabi}
\item \emph{Feedback Recurrent Autoencoder for Video Compression}, shared first-authorship, in review for ACCV 2020. \citep{Golinski2020feedback}
\item \emph{Lossless Compression using Augmented-Space Normalizing Flows}, in exploration phase.
\item \emph{Meta-learning for Probabilistic Inference}, in exploration phase.
\end{enumerate}

In the following section I will briefly describe each of those pieces of work, my contributions, and the outlook of the project.


\subsection{Work completed}

\subsubsection{Faithful Inversion of Generative Models for Effective Amortized Inference}% \citep{Webb2018faithful}}
Work published at NeurIPS 2019, this was my first project after starting my PhD and I was the second author.
I used this work in my Transfer of Status report.
I contributed to this work by coding, running, and analyzing experiments, including some that eventually did not make it to the manuscript. 
I participated in the design of the experiments and the process of writing and reviewing the manuscript.

\subsubsection{Amortized Monte Carlo Integration}% \citep{Golinski2019amci}}

This manuscript constitutes the second part of this report -- you will find it the latter part of this document.

This work was published at the International Conference on Machine Learning (ICML) 2019 and received the Best Paper Runner Up award, one of approximately ten awarded each year.

For this project I was the first author and shared the equal authorship with my supervisor, Tom Rainforth.
My contributions are as follows: we developed the idea jointly with Tom; 
we jointly discussed and decided on the design of the experiments, which I coded and run; 
we shared the effort of writing up of the manuscript for the conference submission and the rebuttal.
I refactored and opensourced the code for the project after the publication.

\subsubsection{Target–Aware Bayesian Inference: How to Beat Optimal Conventional Estimators}% \citep{Rainforth2020tabi}}

This work is an extension of my earlier paper \emph{Amortized Monte Carlo Integration} (AMCI)
and
was published in the Journal of Machine Learning Research (JMLR).

Since this work is an extension of AMCI, its core is the key idea of the AMCI paper we developed together with Tom.
The JMLR paper offered two extensions beyond what was presented in the AMCI paper.
One of those ideas was an idea we came up with together, and the other was Tom's idea.
The execution of both ideas, including the derivation of the theoretical results and the experimental work, has been accomplished entirely by Tom and Sheheryar Zaidi, 
because at the time I have been away on an internship at Qualcomm AI Research in Amsterdam.
For this reason, I was the second author and shared the equal authorship with Tom Rainforth who became the first author on this manuscript.

% Contributions-wise: we have discussed the ideas for the extension of the AMCI paper together with Tom, 
% and one of the two ideas that ended up being the major extension warranting the publication in the JMLR
% (applying the breakdown of the standard importance sampling estimator into multiple parts, suggested in the AMCI paper, but in the Adaptive Importance Sampling rather than Amortized Inference setting)
% was an idea that we came up with together.
% The other of those ideas, the ability to utilize any marginal likelihood estimator to calculate the estimates required for that estimator breakdown, was developed entirely by Tom. 



\subsubsection{Feedback Recurrent Autoencoder for Video Compression}% \citep{Golinski2020feedback}}
I spent 6 months (September 2019 - March 2020) on an internship at Qualcomm AI Research in Amsterdam where I worked on neural video compression.

The output of my internship has been a manuscript that we first submitted to the European Conference on Computer Vision (ECCV) and received a rejection, and now we have resubmitted it to the Asian Conference on Computer Vision (ACCV) and we are awaiting the reviews.
I am the first of three equal first authors.

The nature of our contributions ended up being primarily empirical advances in the design of deep learning architecture for neural video compression.
One of our contributions, that my expertise was particularly useful and relevant for, was an analysis of the prior architectures through the lens of the underlying probabilistic graphical models assumed by the generative and inference processes (decoder and encoder parts of the neural architecture, respectively) making use of the methods I have familiarized myself with and helped develop in \citep{Webb2018faithful}.

I am intending to include this piece of work in my thesis and I am currently awaiting Qualcomm's approval to do that.


\subsection{Work in progress}

\subsubsection{Improving Normalizing Flows via Better Orthogonal Parameterizations}% \citep{Golinski2019orthogonal}}
This work explores using novel, recent methods for performing constrained optimization of the orthogonal matrices \citep{Lezcano2019exprnn} 
used in some normalizing flows, particularly Sylvester Normalizing Flows \citep{vandenBerg2018sylvester}.
The preliminary manuscript has been published at the ICML 2019 Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, and it has been already noticed by the community and cited 5 times at the moment of writing this report.
I am the first of two equal first authors.
We submitted an extended version of the manuscript to NeurIPS 2019 but it was rejected.
% I could not have worked on this project since August 2019 because of my internship at Qualcomm, and in March when I have finished the internship I have been working on opensourcing
At the moment, I am working on extending the empirical investigation to be able to resubmit the paper to one of the conference venues.
I am intending to include this paper in my thesis. 

Contributions-wise: I initiated the project, coded and run the experiments, and written most of the manuscript.

% The relevance of this paper from the fact that normalizing flows are often used as a flexible approximate posterior parameteric probability distribution in amortized variational inference setting.


\subsection{Projects in the exploration phase}

\subsubsection{Lossless Compression using Augmented-Space Normalizing Flows}
This project is meant to explore applying novel variants of normalizing flows to the task of lossless compression.

At the moment, we already formulated a lossless compression scheme for the Continuously Indexed Normalising Flows \citep{Cornish2020cif} through a combination of local bits-back coding \citep{Ho2019local} and BB-ANS \citep{Townsend2019bbans}.
The next step for us is to explore the ways flows fitting into the SurVAE framework \citep{Nielsen2020survae} can be used for the purpose of lossless data compression.

I believe that our current contribution, if we support it by experimental evidence, warrants a workshop publication.
If we make further theoretical advances the project might result in a full conference publication.
Either way, if the project results in a workshop or conference publication I am intending to include it in my thesis.
Most likely, if we maintain the current levels of engagement in the project, I will be one of the two equal first authors of this work, together with Anthony Caterini.

\subsubsection{Meta-learning for Probabilistic Inference}
This project is meant to explore the opportunities to leverage progress in meta-learning literature for the purpose of probabilistic inference.
I am currently at the point of literature review.
The first promising direction I have noticed so far is using a learned optimization method, rather than gradient descent, to refine the proposals in Adaptive Importance Sampling, as has been previously done for Variational Inference in VAEs \citep{Marino2018iterative}.
For now, I am pursuing this project mostly by myself, with input from my supervisor.


\section{Planned steps to completion}

I am planning to finish the experiments and the writeup of \emph{Improving Normalizing Flows via Better Orthogonal Parameterizations} by the submission date of either ICLR 2021 or ICML 2021.


I also expect at least one of the projects, 
\emph{Lossless Compression using Augmented-Space Normalizing Flows} or
\emph{Meta-learning for Probabilistic Inference},
to result in a conference publication. 
Given that I am already working on those projects I hope to submit one of them to either ICML 2021 or NeurIPS 2021.
This means that I will have a manuscript ready to be included in my thesis for the deadline in April 2021. 

I intend to submit an integrated thesis and hence I am not planning much time spent on writing my thesis --
I plan to spend about 2 weeks before my thesis submission deadline in April 2021 to put all of my works together and 
write an appropriate introduction and conclusion giving an overarching narrative to the series of projects I have been working on.


\section{The manuscript}
The manuscript of \emph{Amortized Monte Carlo Integration} starts on the next page.


\bibliographystyle{plainnatnourl}
\bibliography{bibliography/shortstrings,bibliography/bibliography} 


\includepdf[pages=-]{amci}


\end{document}
